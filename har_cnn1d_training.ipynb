{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria repositório de reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.enums import Datas\n",
    "from utils.checkpoints import verifyPath\n",
    "\n",
    "teste_size = 2\n",
    "main_data = Datas.MOTION\n",
    "path_reports = f\"report_results/{Datas.HAR.value}/{main_data.value}_{teste_size}/\"\n",
    "\n",
    "split_path = path_reports.split(\"/\")\n",
    "partial_path = \"\"\n",
    "for i, part in enumerate(split_path):\n",
    "    partial_path += part + \"/\"\n",
    "    verifyPath(partial_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treina Pretexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Hiperparâmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from models.cnn1d import CNN1d\n",
    "from utils.enums import Datas, Sets, ModelTypes\n",
    "from data_modules.pretext import HarDataModule as HarDataModulePretext\n",
    "from transforms.har import rotation, flip, noise_addition, permutation, scaling, time_warp, negation\n",
    "\n",
    "printQtd = 1\n",
    "num_epoch = 320\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optim\n",
    "learning_rate = 0.02\n",
    "step_size = 80\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrega Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [rotation, flip, noise_addition, permutation, scaling, time_warp, negation]\n",
    "data_module = HarDataModulePretext(batch_size=batch_size, main_data = main_data)\n",
    "train_dl, train_ds = data_module.get_dataloader(set=Sets.TRAIN.value, shuffle=True, transforms=transforms)\n",
    "test_dl, test_ds   = data_module.get_dataloader(set=Sets.TEST.value, shuffle=False, transforms=transforms)\n",
    "num_classes = len(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1d(\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (backbone): Backbone(\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv1d(6, 12, kernel_size=(2,), stride=(1,))\n",
      "    (conv2): Conv1d(12, 24, kernel_size=(2,), stride=(1,))\n",
      "    (conv3): Conv1d(24, 48, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (pred_head): ProjectionHead(\n",
      "    (linear1): Linear(in_features=288, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_p = CNN1d(data_label=main_data.value, num_classes=num_classes, require_grad=True, type=ModelTypes.PRETEXT.value)\n",
    "print(model_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sr_rosa/.local/lib/python3.10/site-packages/lightning/pytorch/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/320], Step [ 104/104], Loss: 1.9414 Validation Loss: 1.9451\n",
      "Epoch [   2/320], Step [ 104/104], Loss: 1.9578 Validation Loss: 1.9464\n",
      "Epoch [   3/320], Step [ 104/104], Loss: 1.9331 Validation Loss: 1.9434\n",
      "Epoch [   4/320], Step [ 104/104], Loss: 1.9609 Validation Loss: 1.9438\n",
      "Epoch [   5/320], Step [ 104/104], Loss: 1.9477 Validation Loss: 1.9412\n",
      "Epoch [   6/320], Step [ 104/104], Loss: 1.9508 Validation Loss: 1.9403\n",
      "Epoch [   7/320], Step [ 104/104], Loss: 1.9353 Validation Loss: 1.9382\n",
      "Epoch [   8/320], Step [ 104/104], Loss: 1.9671 Validation Loss: 1.9408\n",
      "Epoch [   9/320], Step [ 104/104], Loss: 1.9364 Validation Loss: 1.9338\n",
      "Epoch [  10/320], Step [ 104/104], Loss: 1.9212 Validation Loss: 1.9293\n",
      "Epoch [  11/320], Step [ 104/104], Loss: 1.9571 Validation Loss: 1.9356\n",
      "Epoch [  12/320], Step [ 104/104], Loss: 1.9127 Validation Loss: 1.9311\n",
      "Epoch [  13/320], Step [ 104/104], Loss: 1.9036 Validation Loss: 1.9250\n",
      "Epoch [  14/320], Step [ 104/104], Loss: 1.9047 Validation Loss: 1.9217\n",
      "Epoch [  15/320], Step [ 104/104], Loss: 1.9438 Validation Loss: 1.8938\n",
      "Epoch [  16/320], Step [ 104/104], Loss: 1.8590 Validation Loss: 1.8763\n",
      "Epoch [  17/320], Step [ 104/104], Loss: 1.8523 Validation Loss: 1.8610\n",
      "Epoch [  18/320], Step [ 104/104], Loss: 1.9065 Validation Loss: 1.8543\n",
      "Epoch [  19/320], Step [ 104/104], Loss: 1.7759 Validation Loss: 1.8381\n",
      "Epoch [  20/320], Step [ 104/104], Loss: 1.8844 Validation Loss: 1.8259\n",
      "Epoch [  21/320], Step [ 104/104], Loss: 1.9318 Validation Loss: 1.8110\n",
      "Epoch [  22/320], Step [ 104/104], Loss: 1.8245 Validation Loss: 1.7890\n",
      "Epoch [  23/320], Step [ 104/104], Loss: 1.7747 Validation Loss: 1.7760\n",
      "Epoch [  24/320], Step [ 104/104], Loss: 1.8220 Validation Loss: 1.7685\n",
      "Epoch [  25/320], Step [ 104/104], Loss: 1.9273 Validation Loss: 1.7404\n",
      "Epoch [  26/320], Step [ 104/104], Loss: 1.6857 Validation Loss: 1.6818\n",
      "Epoch [  27/320], Step [ 104/104], Loss: 1.6984 Validation Loss: 1.6931\n",
      "Epoch [  28/320], Step [ 104/104], Loss: 1.6254 Validation Loss: 1.6631\n",
      "Epoch [  29/320], Step [ 104/104], Loss: 1.6449 Validation Loss: 1.5934\n",
      "Epoch [  30/320], Step [ 104/104], Loss: 1.6392 Validation Loss: 1.6363\n",
      "Epoch [  31/320], Step [ 104/104], Loss: 1.6145 Validation Loss: 1.6102\n",
      "Epoch [  32/320], Step [ 104/104], Loss: 1.3192 Validation Loss: 1.6241\n",
      "Epoch [  33/320], Step [ 104/104], Loss: 1.6880 Validation Loss: 1.5819\n",
      "Epoch [  34/320], Step [ 104/104], Loss: 1.3809 Validation Loss: 1.5423\n",
      "Epoch [  35/320], Step [ 104/104], Loss: 1.6636 Validation Loss: 1.5574\n",
      "Epoch [  36/320], Step [ 104/104], Loss: 1.4282 Validation Loss: 1.5361\n",
      "Epoch [  37/320], Step [ 104/104], Loss: 1.6135 Validation Loss: 1.5277\n",
      "Epoch [  38/320], Step [ 104/104], Loss: 1.7424 Validation Loss: 1.5083\n",
      "Epoch [  39/320], Step [ 104/104], Loss: 1.3675 Validation Loss: 1.4574\n",
      "Epoch [  40/320], Step [ 104/104], Loss: 1.1325 Validation Loss: 1.4608\n",
      "Epoch [  41/320], Step [ 104/104], Loss: 1.5074 Validation Loss: 1.6376\n",
      "Epoch [  42/320], Step [ 104/104], Loss: 1.5469 Validation Loss: 1.4678\n",
      "Epoch [  43/320], Step [ 104/104], Loss: 1.3698 Validation Loss: 1.4605\n",
      "Epoch [  44/320], Step [ 104/104], Loss: 1.1359 Validation Loss: 1.4395\n",
      "Epoch [  45/320], Step [ 104/104], Loss: 1.4578 Validation Loss: 1.3891\n",
      "Epoch [  46/320], Step [ 104/104], Loss: 1.3689 Validation Loss: 1.3919\n",
      "Epoch [  47/320], Step [ 104/104], Loss: 1.0285 Validation Loss: 1.4101\n",
      "Epoch [  48/320], Step [ 104/104], Loss: 1.5572 Validation Loss: 1.5357\n",
      "Epoch [  49/320], Step [ 104/104], Loss: 1.7280 Validation Loss: 1.3060\n",
      "Epoch [  50/320], Step [ 104/104], Loss: 1.4262 Validation Loss: 1.2843\n",
      "Epoch [  51/320], Step [ 104/104], Loss: 1.3875 Validation Loss: 1.3235\n",
      "Epoch [  52/320], Step [ 104/104], Loss: 1.2829 Validation Loss: 1.4161\n",
      "Epoch [  53/320], Step [ 104/104], Loss: 1.4892 Validation Loss: 1.3685\n",
      "Epoch [  54/320], Step [ 104/104], Loss: 1.4790 Validation Loss: 1.3486\n",
      "Epoch [  55/320], Step [ 104/104], Loss: 1.2319 Validation Loss: 1.3104\n",
      "Epoch [  56/320], Step [ 104/104], Loss: 1.1962 Validation Loss: 1.2323\n",
      "Epoch [  57/320], Step [ 104/104], Loss: 0.9457 Validation Loss: 1.2895\n",
      "Epoch [  58/320], Step [ 104/104], Loss: 1.0127 Validation Loss: 1.2953\n",
      "Epoch [  59/320], Step [ 104/104], Loss: 1.1864 Validation Loss: 1.2855\n",
      "Epoch [  60/320], Step [ 104/104], Loss: 1.0254 Validation Loss: 1.2387\n",
      "Epoch [  61/320], Step [ 104/104], Loss: 1.2413 Validation Loss: 1.2205\n",
      "Epoch [  62/320], Step [ 104/104], Loss: 1.3744 Validation Loss: 1.3789\n",
      "Epoch [  63/320], Step [ 104/104], Loss: 0.9460 Validation Loss: 1.2012\n",
      "Epoch [  64/320], Step [ 104/104], Loss: 1.0656 Validation Loss: 1.2803\n",
      "Epoch [  65/320], Step [ 104/104], Loss: 1.3062 Validation Loss: 1.1900\n",
      "Epoch [  66/320], Step [ 104/104], Loss: 1.4475 Validation Loss: 1.5206\n",
      "Epoch [  67/320], Step [ 104/104], Loss: 1.1219 Validation Loss: 1.1438\n",
      "Epoch [  68/320], Step [ 104/104], Loss: 1.1325 Validation Loss: 1.2512\n",
      "Epoch [  69/320], Step [ 104/104], Loss: 1.1795 Validation Loss: 1.1742\n",
      "Epoch [  70/320], Step [ 104/104], Loss: 0.5796 Validation Loss: 1.1964\n",
      "Epoch [  71/320], Step [ 104/104], Loss: 1.1008 Validation Loss: 1.0922\n",
      "Epoch [  72/320], Step [ 104/104], Loss: 1.1476 Validation Loss: 1.1681\n",
      "Epoch [  73/320], Step [ 104/104], Loss: 0.9766 Validation Loss: 1.1846\n",
      "Epoch [  74/320], Step [ 104/104], Loss: 1.3684 Validation Loss: 1.0455\n",
      "Epoch [  75/320], Step [ 104/104], Loss: 0.9541 Validation Loss: 1.1001\n",
      "Epoch [  76/320], Step [ 104/104], Loss: 0.9965 Validation Loss: 1.1141\n",
      "Epoch [  77/320], Step [ 104/104], Loss: 1.1428 Validation Loss: 1.0838\n",
      "Epoch [  78/320], Step [ 104/104], Loss: 1.5380 Validation Loss: 1.0951\n",
      "Epoch [  79/320], Step [ 104/104], Loss: 1.3323 Validation Loss: 1.0938\n",
      "Epoch [  80/320], Step [ 104/104], Loss: 0.9575 Validation Loss: 1.0613\n",
      "Epoch [  81/320], Step [ 104/104], Loss: 1.1504 Validation Loss: 1.0592\n",
      "Epoch [  82/320], Step [ 104/104], Loss: 1.2686 Validation Loss: 1.0358\n",
      "Epoch [  83/320], Step [ 104/104], Loss: 0.8623 Validation Loss: 1.0395\n",
      "Epoch [  84/320], Step [ 104/104], Loss: 1.0704 Validation Loss: 1.0667\n",
      "Epoch [  85/320], Step [ 104/104], Loss: 0.7750 Validation Loss: 1.0232\n",
      "Epoch [  86/320], Step [ 104/104], Loss: 1.1477 Validation Loss: 0.9956\n",
      "Epoch [  87/320], Step [ 104/104], Loss: 1.1750 Validation Loss: 1.0147\n",
      "Epoch [  88/320], Step [ 104/104], Loss: 0.8623 Validation Loss: 1.0707\n",
      "Epoch [  89/320], Step [ 104/104], Loss: 1.4624 Validation Loss: 1.0735\n",
      "Epoch [  90/320], Step [ 104/104], Loss: 0.9095 Validation Loss: 1.0236\n",
      "Epoch [  91/320], Step [ 104/104], Loss: 0.9980 Validation Loss: 0.9877\n",
      "Epoch [  92/320], Step [ 104/104], Loss: 1.0034 Validation Loss: 1.0018\n",
      "Epoch [  93/320], Step [ 104/104], Loss: 1.1190 Validation Loss: 0.9785\n",
      "Epoch [  94/320], Step [ 104/104], Loss: 0.9221 Validation Loss: 1.0025\n",
      "Epoch [  95/320], Step [ 104/104], Loss: 0.8301 Validation Loss: 0.9727\n",
      "Epoch [  96/320], Step [ 104/104], Loss: 0.6774 Validation Loss: 1.0077\n",
      "Epoch [  97/320], Step [ 104/104], Loss: 1.3248 Validation Loss: 1.0693\n",
      "Epoch [  98/320], Step [ 104/104], Loss: 0.9647 Validation Loss: 0.9608\n",
      "Epoch [  99/320], Step [ 104/104], Loss: 0.8254 Validation Loss: 0.9453\n",
      "Epoch [ 100/320], Step [ 104/104], Loss: 1.1648 Validation Loss: 1.0043\n",
      "Epoch [ 101/320], Step [ 104/104], Loss: 0.9184 Validation Loss: 0.9648\n",
      "Epoch [ 102/320], Step [ 104/104], Loss: 0.7892 Validation Loss: 0.9675\n",
      "Epoch [ 103/320], Step [ 104/104], Loss: 0.9502 Validation Loss: 0.9583\n",
      "Epoch [ 104/320], Step [ 104/104], Loss: 1.3325 Validation Loss: 0.9765\n",
      "Epoch [ 105/320], Step [ 104/104], Loss: 1.2837 Validation Loss: 0.9646\n",
      "Epoch [ 106/320], Step [ 104/104], Loss: 1.0005 Validation Loss: 0.9708\n",
      "Epoch [ 107/320], Step [ 104/104], Loss: 1.2098 Validation Loss: 0.9689\n",
      "Epoch [ 108/320], Step [ 104/104], Loss: 0.9949 Validation Loss: 0.9788\n",
      "Epoch [ 109/320], Step [ 104/104], Loss: 1.2203 Validation Loss: 0.9534\n",
      "Epoch [ 110/320], Step [ 104/104], Loss: 0.7996 Validation Loss: 0.9451\n",
      "Epoch [ 111/320], Step [ 104/104], Loss: 1.2293 Validation Loss: 0.9366\n",
      "Epoch [ 112/320], Step [ 104/104], Loss: 0.8408 Validation Loss: 1.0269\n",
      "Epoch [ 113/320], Step [ 104/104], Loss: 1.0373 Validation Loss: 1.0083\n",
      "Epoch [ 114/320], Step [ 104/104], Loss: 1.0243 Validation Loss: 0.9295\n",
      "Epoch [ 115/320], Step [ 104/104], Loss: 0.7443 Validation Loss: 0.9249\n",
      "Epoch [ 116/320], Step [ 104/104], Loss: 0.9353 Validation Loss: 0.9057\n",
      "Epoch [ 117/320], Step [ 104/104], Loss: 0.9678 Validation Loss: 0.8959\n",
      "Epoch [ 118/320], Step [ 104/104], Loss: 0.6200 Validation Loss: 0.9137\n",
      "Epoch [ 119/320], Step [ 104/104], Loss: 1.0241 Validation Loss: 0.8796\n",
      "Epoch [ 120/320], Step [ 104/104], Loss: 0.9400 Validation Loss: 0.8878\n",
      "Epoch [ 121/320], Step [ 104/104], Loss: 1.0164 Validation Loss: 0.9384\n",
      "Epoch [ 122/320], Step [ 104/104], Loss: 0.8423 Validation Loss: 0.9142\n",
      "Epoch [ 123/320], Step [ 104/104], Loss: 0.9857 Validation Loss: 0.8845\n",
      "Epoch [ 124/320], Step [ 104/104], Loss: 0.6511 Validation Loss: 0.9016\n",
      "Epoch [ 125/320], Step [ 104/104], Loss: 0.9043 Validation Loss: 1.0221\n",
      "Epoch [ 126/320], Step [ 104/104], Loss: 1.0130 Validation Loss: 0.9652\n",
      "Epoch [ 127/320], Step [ 104/104], Loss: 0.9372 Validation Loss: 0.9031\n",
      "Epoch [ 128/320], Step [ 104/104], Loss: 1.0407 Validation Loss: 0.9122\n",
      "Epoch [ 129/320], Step [ 104/104], Loss: 1.2950 Validation Loss: 0.8995\n",
      "Epoch [ 130/320], Step [ 104/104], Loss: 0.9830 Validation Loss: 0.8943\n",
      "Epoch [ 131/320], Step [ 104/104], Loss: 0.7461 Validation Loss: 0.9383\n",
      "Epoch [ 132/320], Step [ 104/104], Loss: 1.0052 Validation Loss: 0.9345\n",
      "Epoch [ 133/320], Step [ 104/104], Loss: 0.6090 Validation Loss: 0.9472\n",
      "Epoch [ 134/320], Step [ 104/104], Loss: 0.6347 Validation Loss: 0.8634\n",
      "Epoch [ 135/320], Step [ 104/104], Loss: 1.0076 Validation Loss: 0.8798\n",
      "Epoch [ 136/320], Step [ 104/104], Loss: 0.7039 Validation Loss: 0.8735\n",
      "Epoch [ 137/320], Step [ 104/104], Loss: 1.0683 Validation Loss: 0.9413\n",
      "Epoch [ 138/320], Step [ 104/104], Loss: 0.8955 Validation Loss: 0.8402\n",
      "Epoch [ 139/320], Step [ 104/104], Loss: 0.9110 Validation Loss: 0.8855\n",
      "Epoch [ 140/320], Step [ 104/104], Loss: 0.8070 Validation Loss: 0.9064\n",
      "Epoch [ 141/320], Step [ 104/104], Loss: 0.8898 Validation Loss: 0.8946\n",
      "Epoch [ 142/320], Step [ 104/104], Loss: 0.8867 Validation Loss: 0.9224\n",
      "Epoch [ 143/320], Step [ 104/104], Loss: 1.1305 Validation Loss: 1.0443\n",
      "Epoch [ 144/320], Step [ 104/104], Loss: 1.0461 Validation Loss: 0.8936\n",
      "Epoch [ 145/320], Step [ 104/104], Loss: 0.9950 Validation Loss: 0.8647\n",
      "Epoch [ 146/320], Step [ 104/104], Loss: 0.8182 Validation Loss: 0.8704\n",
      "Epoch [ 147/320], Step [ 104/104], Loss: 0.6582 Validation Loss: 0.8298\n",
      "Epoch [ 148/320], Step [ 104/104], Loss: 0.7790 Validation Loss: 0.9115\n",
      "Epoch [ 149/320], Step [ 104/104], Loss: 0.5147 Validation Loss: 0.8682\n",
      "Epoch [ 150/320], Step [ 104/104], Loss: 0.7959 Validation Loss: 0.8576\n",
      "Epoch [ 151/320], Step [ 104/104], Loss: 0.9009 Validation Loss: 0.9336\n",
      "Epoch [ 152/320], Step [ 104/104], Loss: 0.9294 Validation Loss: 0.8965\n",
      "Epoch [ 153/320], Step [ 104/104], Loss: 0.9777 Validation Loss: 0.8379\n",
      "Epoch [ 154/320], Step [ 104/104], Loss: 0.7138 Validation Loss: 0.8180\n",
      "Epoch [ 155/320], Step [ 104/104], Loss: 1.8016 Validation Loss: 0.9825\n",
      "Epoch [ 156/320], Step [ 104/104], Loss: 0.8407 Validation Loss: 0.8346\n",
      "Epoch [ 157/320], Step [ 104/104], Loss: 1.1967 Validation Loss: 0.8907\n",
      "Epoch [ 158/320], Step [ 104/104], Loss: 0.7597 Validation Loss: 0.9182\n",
      "Epoch [ 159/320], Step [ 104/104], Loss: 0.8092 Validation Loss: 0.8863\n",
      "Epoch [ 160/320], Step [ 104/104], Loss: 0.9202 Validation Loss: 0.8105\n",
      "Epoch [ 161/320], Step [ 104/104], Loss: 0.7089 Validation Loss: 0.7991\n",
      "Epoch [ 162/320], Step [ 104/104], Loss: 0.8949 Validation Loss: 0.8292\n",
      "Epoch [ 163/320], Step [ 104/104], Loss: 0.7427 Validation Loss: 0.7896\n",
      "Epoch [ 164/320], Step [ 104/104], Loss: 0.8395 Validation Loss: 0.8523\n",
      "Epoch [ 165/320], Step [ 104/104], Loss: 1.1096 Validation Loss: 0.8485\n",
      "Epoch [ 166/320], Step [ 104/104], Loss: 0.8619 Validation Loss: 0.8256\n",
      "Epoch [ 167/320], Step [ 104/104], Loss: 0.3595 Validation Loss: 0.8080\n",
      "Epoch [ 168/320], Step [ 104/104], Loss: 0.7605 Validation Loss: 0.8244\n",
      "Epoch [ 169/320], Step [ 104/104], Loss: 0.8447 Validation Loss: 0.7937\n",
      "Epoch [ 170/320], Step [ 104/104], Loss: 0.8848 Validation Loss: 0.8361\n",
      "Epoch [ 171/320], Step [ 104/104], Loss: 0.9727 Validation Loss: 0.8357\n",
      "Epoch [ 172/320], Step [ 104/104], Loss: 0.6206 Validation Loss: 0.8392\n",
      "Epoch [ 173/320], Step [ 104/104], Loss: 0.9037 Validation Loss: 0.8376\n",
      "Epoch [ 174/320], Step [ 104/104], Loss: 1.0306 Validation Loss: 0.8397\n",
      "Epoch [ 175/320], Step [ 104/104], Loss: 1.0022 Validation Loss: 0.8743\n",
      "Epoch [ 176/320], Step [ 104/104], Loss: 0.8266 Validation Loss: 0.7755\n",
      "Epoch [ 177/320], Step [ 104/104], Loss: 0.9011 Validation Loss: 0.7909\n",
      "Epoch [ 178/320], Step [ 104/104], Loss: 0.8236 Validation Loss: 0.7879\n",
      "Epoch [ 179/320], Step [ 104/104], Loss: 0.6407 Validation Loss: 0.8199\n",
      "Epoch [ 180/320], Step [ 104/104], Loss: 1.1486 Validation Loss: 0.7788\n",
      "Epoch [ 181/320], Step [ 104/104], Loss: 0.5191 Validation Loss: 0.8303\n",
      "Epoch [ 182/320], Step [ 104/104], Loss: 0.6625 Validation Loss: 0.8075\n",
      "Epoch [ 183/320], Step [ 104/104], Loss: 0.4461 Validation Loss: 0.7967\n",
      "Epoch [ 184/320], Step [ 104/104], Loss: 0.7855 Validation Loss: 0.8183\n",
      "Epoch [ 185/320], Step [ 104/104], Loss: 0.6614 Validation Loss: 0.8066\n",
      "Epoch [ 186/320], Step [ 104/104], Loss: 0.6080 Validation Loss: 0.8007\n",
      "Epoch [ 187/320], Step [ 104/104], Loss: 0.8850 Validation Loss: 0.8616\n",
      "Epoch [ 188/320], Step [ 104/104], Loss: 0.7404 Validation Loss: 0.7863\n",
      "Epoch [ 189/320], Step [ 104/104], Loss: 0.7721 Validation Loss: 0.8422\n",
      "Epoch [ 190/320], Step [ 104/104], Loss: 0.6971 Validation Loss: 0.8006\n",
      "Epoch [ 191/320], Step [ 104/104], Loss: 0.7533 Validation Loss: 0.8686\n",
      "Epoch [ 192/320], Step [ 104/104], Loss: 1.4691 Validation Loss: 0.8248\n",
      "Epoch [ 193/320], Step [ 104/104], Loss: 0.5820 Validation Loss: 0.8776\n",
      "Epoch [ 194/320], Step [ 104/104], Loss: 0.6821 Validation Loss: 0.7773\n",
      "Epoch [ 195/320], Step [ 104/104], Loss: 0.8948 Validation Loss: 0.7705\n",
      "Epoch [ 196/320], Step [ 104/104], Loss: 0.9631 Validation Loss: 0.7595\n",
      "Epoch [ 197/320], Step [ 104/104], Loss: 0.7688 Validation Loss: 0.7768\n",
      "Epoch [ 198/320], Step [ 104/104], Loss: 0.6769 Validation Loss: 0.7978\n",
      "Epoch [ 199/320], Step [ 104/104], Loss: 0.9854 Validation Loss: 0.7875\n",
      "Epoch [ 200/320], Step [ 104/104], Loss: 1.0535 Validation Loss: 0.7710\n",
      "Epoch [ 201/320], Step [ 104/104], Loss: 0.7678 Validation Loss: 0.7636\n",
      "Epoch [ 202/320], Step [ 104/104], Loss: 0.8511 Validation Loss: 0.8039\n",
      "Epoch [ 203/320], Step [ 104/104], Loss: 0.9606 Validation Loss: 0.7554\n",
      "Epoch [ 204/320], Step [ 104/104], Loss: 1.0859 Validation Loss: 0.8304\n",
      "Epoch [ 205/320], Step [ 104/104], Loss: 0.7472 Validation Loss: 0.7855\n",
      "Epoch [ 206/320], Step [ 104/104], Loss: 0.9957 Validation Loss: 0.7550\n",
      "Epoch [ 207/320], Step [ 104/104], Loss: 0.7431 Validation Loss: 0.7444\n",
      "Epoch [ 208/320], Step [ 104/104], Loss: 0.5733 Validation Loss: 0.7685\n",
      "Epoch [ 209/320], Step [ 104/104], Loss: 0.5801 Validation Loss: 0.7531\n",
      "Epoch [ 210/320], Step [ 104/104], Loss: 0.6692 Validation Loss: 0.7972\n",
      "Epoch [ 211/320], Step [ 104/104], Loss: 0.8404 Validation Loss: 0.8088\n",
      "Epoch [ 212/320], Step [ 104/104], Loss: 0.6422 Validation Loss: 0.7580\n",
      "Epoch [ 213/320], Step [ 104/104], Loss: 0.7584 Validation Loss: 0.7506\n",
      "Epoch [ 214/320], Step [ 104/104], Loss: 0.6374 Validation Loss: 0.7750\n",
      "Epoch [ 215/320], Step [ 104/104], Loss: 0.7003 Validation Loss: 0.7667\n",
      "Epoch [ 216/320], Step [ 104/104], Loss: 0.9534 Validation Loss: 0.7377\n",
      "Epoch [ 217/320], Step [ 104/104], Loss: 0.9890 Validation Loss: 0.8664\n",
      "Epoch [ 218/320], Step [ 104/104], Loss: 1.2466 Validation Loss: 0.7703\n",
      "Epoch [ 219/320], Step [ 104/104], Loss: 0.5597 Validation Loss: 0.8178\n",
      "Epoch [ 220/320], Step [ 104/104], Loss: 0.8678 Validation Loss: 0.7574\n",
      "Epoch [ 221/320], Step [ 104/104], Loss: 0.7781 Validation Loss: 0.7773\n",
      "Epoch [ 222/320], Step [ 104/104], Loss: 0.7399 Validation Loss: 0.7299\n",
      "Epoch [ 223/320], Step [ 104/104], Loss: 0.6933 Validation Loss: 0.7365\n",
      "Epoch [ 224/320], Step [ 104/104], Loss: 0.5203 Validation Loss: 0.8088\n",
      "Epoch [ 225/320], Step [ 104/104], Loss: 0.6817 Validation Loss: 0.7808\n",
      "Epoch [ 226/320], Step [ 104/104], Loss: 0.9006 Validation Loss: 0.8714\n",
      "Epoch [ 227/320], Step [ 104/104], Loss: 0.9339 Validation Loss: 0.7736\n",
      "Epoch [ 228/320], Step [ 104/104], Loss: 0.9227 Validation Loss: 0.7693\n",
      "Epoch [ 229/320], Step [ 104/104], Loss: 0.6295 Validation Loss: 0.7578\n",
      "Epoch [ 230/320], Step [ 104/104], Loss: 0.9621 Validation Loss: 0.8121\n",
      "Epoch [ 231/320], Step [ 104/104], Loss: 0.5114 Validation Loss: 0.8126\n",
      "Epoch [ 232/320], Step [ 104/104], Loss: 0.8857 Validation Loss: 0.7547\n",
      "Epoch [ 233/320], Step [ 104/104], Loss: 1.1356 Validation Loss: 0.7574\n",
      "Epoch [ 234/320], Step [ 104/104], Loss: 0.9039 Validation Loss: 0.7881\n",
      "Epoch [ 235/320], Step [ 104/104], Loss: 0.6285 Validation Loss: 0.7814\n",
      "Epoch [ 236/320], Step [ 104/104], Loss: 1.0616 Validation Loss: 0.7976\n",
      "Epoch [ 237/320], Step [ 104/104], Loss: 0.8425 Validation Loss: 0.7868\n",
      "Epoch [ 238/320], Step [ 104/104], Loss: 0.5174 Validation Loss: 0.7781\n",
      "Epoch [ 239/320], Step [ 104/104], Loss: 0.7833 Validation Loss: 0.7320\n",
      "Epoch [ 240/320], Step [ 104/104], Loss: 0.4103 Validation Loss: 0.7750\n",
      "Epoch [ 241/320], Step [ 104/104], Loss: 0.9408 Validation Loss: 0.7698\n",
      "Epoch [ 242/320], Step [ 104/104], Loss: 0.8806 Validation Loss: 0.7694\n",
      "Epoch [ 243/320], Step [ 104/104], Loss: 0.9378 Validation Loss: 0.7726\n",
      "Epoch [ 244/320], Step [ 104/104], Loss: 0.9353 Validation Loss: 0.7511\n",
      "Epoch [ 245/320], Step [ 104/104], Loss: 0.8591 Validation Loss: 0.7433\n",
      "Epoch [ 246/320], Step [ 104/104], Loss: 0.6954 Validation Loss: 0.7082\n",
      "Epoch [ 247/320], Step [ 104/104], Loss: 1.1676 Validation Loss: 0.7489\n",
      "Epoch [ 248/320], Step [ 104/104], Loss: 0.7630 Validation Loss: 0.7467\n",
      "Epoch [ 249/320], Step [ 104/104], Loss: 0.4791 Validation Loss: 0.7158\n",
      "Epoch [ 250/320], Step [ 104/104], Loss: 0.8431 Validation Loss: 0.7551\n",
      "Epoch [ 251/320], Step [ 104/104], Loss: 0.9046 Validation Loss: 0.7212\n",
      "Epoch [ 252/320], Step [ 104/104], Loss: 0.7755 Validation Loss: 0.7352\n",
      "Epoch [ 253/320], Step [ 104/104], Loss: 0.3650 Validation Loss: 0.7546\n",
      "Epoch [ 254/320], Step [ 104/104], Loss: 0.6755 Validation Loss: 0.7315\n",
      "Epoch [ 255/320], Step [ 104/104], Loss: 0.5055 Validation Loss: 0.7604\n",
      "Epoch [ 256/320], Step [ 104/104], Loss: 0.9697 Validation Loss: 0.7543\n",
      "Epoch [ 257/320], Step [ 104/104], Loss: 0.6168 Validation Loss: 0.7943\n",
      "Epoch [ 258/320], Step [ 104/104], Loss: 0.9780 Validation Loss: 0.7550\n",
      "Epoch [ 259/320], Step [ 104/104], Loss: 0.6607 Validation Loss: 0.7720\n",
      "Epoch [ 260/320], Step [ 104/104], Loss: 0.9881 Validation Loss: 0.7705\n",
      "Epoch [ 261/320], Step [ 104/104], Loss: 0.8244 Validation Loss: 0.7293\n",
      "Epoch [ 262/320], Step [ 104/104], Loss: 0.6813 Validation Loss: 0.7358\n",
      "Epoch [ 263/320], Step [ 104/104], Loss: 0.6355 Validation Loss: 0.7479\n",
      "Epoch [ 264/320], Step [ 104/104], Loss: 0.5963 Validation Loss: 0.7222\n",
      "Epoch [ 265/320], Step [ 104/104], Loss: 1.4213 Validation Loss: 0.7647\n",
      "Epoch [ 266/320], Step [ 104/104], Loss: 0.5785 Validation Loss: 0.7383\n",
      "Epoch [ 267/320], Step [ 104/104], Loss: 0.6948 Validation Loss: 0.6985\n",
      "Epoch [ 268/320], Step [ 104/104], Loss: 0.6747 Validation Loss: 0.7339\n",
      "Epoch [ 269/320], Step [ 104/104], Loss: 0.6913 Validation Loss: 0.7074\n",
      "Epoch [ 270/320], Step [ 104/104], Loss: 0.4014 Validation Loss: 0.7417\n",
      "Epoch [ 271/320], Step [ 104/104], Loss: 0.6107 Validation Loss: 0.7432\n",
      "Epoch [ 272/320], Step [ 104/104], Loss: 0.2483 Validation Loss: 0.7394\n",
      "Epoch [ 273/320], Step [ 104/104], Loss: 0.6704 Validation Loss: 0.7681\n",
      "Epoch [ 274/320], Step [ 104/104], Loss: 0.8404 Validation Loss: 0.7188\n",
      "Epoch [ 275/320], Step [ 104/104], Loss: 0.6794 Validation Loss: 0.7628\n",
      "Epoch [ 276/320], Step [ 104/104], Loss: 0.7477 Validation Loss: 0.7619\n",
      "Epoch [ 277/320], Step [ 104/104], Loss: 0.7750 Validation Loss: 0.7068\n",
      "Epoch [ 278/320], Step [ 104/104], Loss: 0.6779 Validation Loss: 0.7342\n",
      "Epoch [ 279/320], Step [ 104/104], Loss: 0.8298 Validation Loss: 0.7566\n",
      "Epoch [ 280/320], Step [ 104/104], Loss: 0.6300 Validation Loss: 0.7324\n",
      "Epoch [ 281/320], Step [ 104/104], Loss: 0.8471 Validation Loss: 0.7709\n",
      "Epoch [ 282/320], Step [ 104/104], Loss: 0.7751 Validation Loss: 0.7378\n",
      "Epoch [ 283/320], Step [ 104/104], Loss: 0.9626 Validation Loss: 0.7449\n",
      "Epoch [ 284/320], Step [ 104/104], Loss: 0.4011 Validation Loss: 0.7628\n",
      "Epoch [ 285/320], Step [ 104/104], Loss: 0.9773 Validation Loss: 0.7202\n",
      "Epoch [ 286/320], Step [ 104/104], Loss: 0.5135 Validation Loss: 0.6868\n",
      "Epoch [ 287/320], Step [ 104/104], Loss: 0.4916 Validation Loss: 0.7308\n",
      "Epoch [ 288/320], Step [ 104/104], Loss: 0.5959 Validation Loss: 0.7157\n",
      "Epoch [ 289/320], Step [ 104/104], Loss: 0.5380 Validation Loss: 0.7032\n",
      "Epoch [ 290/320], Step [ 104/104], Loss: 1.2782 Validation Loss: 0.7088\n",
      "Epoch [ 291/320], Step [ 104/104], Loss: 0.4855 Validation Loss: 0.7066\n",
      "Epoch [ 292/320], Step [ 104/104], Loss: 0.6568 Validation Loss: 0.7088\n",
      "Epoch [ 293/320], Step [ 104/104], Loss: 0.4828 Validation Loss: 0.7017\n",
      "Epoch [ 294/320], Step [ 104/104], Loss: 0.7998 Validation Loss: 0.7157\n",
      "Epoch [ 295/320], Step [ 104/104], Loss: 0.5501 Validation Loss: 0.7048\n",
      "Epoch [ 296/320], Step [ 104/104], Loss: 0.8609 Validation Loss: 0.7515\n",
      "Epoch [ 297/320], Step [ 104/104], Loss: 1.1145 Validation Loss: 0.7621\n",
      "Epoch [ 298/320], Step [ 104/104], Loss: 0.6118 Validation Loss: 0.7764\n",
      "Epoch [ 299/320], Step [ 104/104], Loss: 0.7499 Validation Loss: 0.7085\n",
      "Epoch [ 300/320], Step [ 104/104], Loss: 0.4683 Validation Loss: 0.7182\n",
      "Epoch [ 301/320], Step [ 104/104], Loss: 0.8284 Validation Loss: 0.7365\n",
      "Epoch [ 302/320], Step [ 104/104], Loss: 1.0737 Validation Loss: 0.7025\n",
      "Epoch [ 303/320], Step [ 104/104], Loss: 0.7526 Validation Loss: 0.7442\n",
      "Epoch [ 304/320], Step [ 104/104], Loss: 0.7421 Validation Loss: 0.7082\n",
      "Epoch [ 305/320], Step [ 104/104], Loss: 0.7603 Validation Loss: 0.7045\n",
      "Epoch [ 306/320], Step [ 104/104], Loss: 0.8996 Validation Loss: 0.7435\n",
      "Epoch [ 307/320], Step [ 104/104], Loss: 1.1872 Validation Loss: 0.6525\n",
      "Epoch [ 308/320], Step [ 104/104], Loss: 0.9639 Validation Loss: 0.7439\n",
      "Epoch [ 309/320], Step [ 104/104], Loss: 0.6147 Validation Loss: 0.7196\n",
      "Epoch [ 310/320], Step [ 104/104], Loss: 0.6599 Validation Loss: 0.7170\n",
      "Epoch [ 311/320], Step [ 104/104], Loss: 0.6898 Validation Loss: 0.7508\n",
      "Epoch [ 312/320], Step [ 104/104], Loss: 0.7533 Validation Loss: 0.7188\n",
      "Epoch [ 313/320], Step [ 104/104], Loss: 0.7604 Validation Loss: 0.7133\n",
      "Epoch [ 314/320], Step [ 104/104], Loss: 1.0281 Validation Loss: 0.7372\n",
      "Epoch [ 315/320], Step [ 104/104], Loss: 0.6103 Validation Loss: 0.7171\n",
      "Epoch [ 316/320], Step [ 104/104], Loss: 0.6555 Validation Loss: 0.7432\n",
      "Epoch [ 317/320], Step [ 104/104], Loss: 0.7089 Validation Loss: 0.7470\n",
      "Epoch [ 318/320], Step [ 104/104], Loss: 0.5941 Validation Loss: 0.6969\n",
      "Epoch [ 319/320], Step [ 104/104], Loss: 0.6438 Validation Loss: 0.7375\n",
      "Epoch [ 320/320], Step [ 104/104], Loss: 0.4349 Validation Loss: 0.7362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import lightning as L\n",
    "# trainer = L.Trainer(\n",
    "#     max_epochs=10,\n",
    "#     accelerator='cpu',\n",
    "#     log_every_n_steps=1        \n",
    "# )\n",
    "# trainer.fit(model=model, train_dataloaders=train_dl)\n",
    "\n",
    "optimizer, lr_scheduler = model_p.configure_optimizers(step_size=step_size, gamma=gamma, learning_rate=learning_rate)\n",
    "optimizer, lr_scheduler = optimizer[0], lr_scheduler[0]\n",
    "\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "best_val_loss = 500\n",
    "n_total_steps = len(train_dl)\n",
    "for epoch in range(num_epoch):\n",
    "    # Treinamento\n",
    "    model_p.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        loss = model_p.training_step(batch)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % math.floor(n_total_steps/printQtd) == 0:\n",
    "            print (f'Epoch [{epoch+1:4d}/{num_epoch}], Step [{i+1:4d}/{n_total_steps}], Loss: {loss.item():.4f}', end= \"\" if n_total_steps/printQtd+i >= n_total_steps else \"\\n\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Validação\n",
    "    model_p.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            val_loss += model_p.validation_step(batch)\n",
    "    \n",
    "    val_loss /= len(test_dl)\n",
    "    print(f' Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    train_errors.append(train_loss/len(train_dl))\n",
    "    validation_errors.append(val_loss.item())\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model_p.state_dict()  # Salva os parâmetros do modelo\n",
    "\n",
    "model_p.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 74.56078706957133 %\n",
      "Accuracy of Rotation (137/206 | 153): 66.50485436893204 %\n",
      "Accuracy of Flip (137/203 | 181): 67.48768472906404 %\n",
      "Accuracy of Noise_addition (192/209 | 245): 91.86602870813397 %\n",
      "Accuracy of Permutation (156/209 | 249): 74.64114832535886 %\n",
      "Accuracy of Scaling (140/195 | 164): 71.7948717948718 %\n",
      "Accuracy of Time Warp (115/203 | 145): 56.65024630541872 %\n",
      "Accuracy of Negation (184/198 | 286): 92.92929292929293 %\n"
     ]
    }
   ],
   "source": [
    "accTotal = 0\n",
    "predicted_values = []\n",
    "real_values = []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(num_classes)]\n",
    "    n_class_samples = [0 for i in range(num_classes)]\n",
    "    n_each_class_samples = [0 for i in range(num_classes)]\n",
    "\n",
    "    for data, labels in test_dl:\n",
    "        outputs = model_p(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for pred, real in zip (predicted, labels):\n",
    "            predicted_values.append(pred.item())\n",
    "            real_values.append(real.item())\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            label = labels[i]\n",
    "            pred  = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            n_each_class_samples[pred] += 1\n",
    "\n",
    "    accTotal = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {accTotal} %')\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {test_ds.getLabel(i)} ({n_class_correct[i]}/{n_class_samples[i]} | {n_each_class_samples[i]}): {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salva resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p.save_backbone(accuracy=accTotal, batch_size=batch_size, num_epoch=num_epoch)\n",
    "\n",
    "pred_reports = pd.DataFrame({\n",
    "    Sets.REAL.value: real_values,\n",
    "    Sets.PREDICTION.value : predicted_values\n",
    "})\n",
    "pred_reports.to_csv(f\"{path_reports}/predictions_{ModelTypes.PRETEXT.value}.dat\", sep=\" \", index=False)\n",
    "\n",
    "train_reports = pd.DataFrame({\n",
    "    Sets.TRAIN.value : train_errors,\n",
    "    Sets.VALIDATION.value : validation_errors\n",
    "})\n",
    "train_reports.to_csv(f\"{path_reports}/errors_{ModelTypes.PRETEXT.value}.dat\", sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treina Downstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperparâmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_modules.har import HarDataModule as HarDataModuleDownstram\n",
    "from utils.enums import Datas, Sets, ModelTypes\n",
    "from models.cnn1d import CNN1d\n",
    "\n",
    "printQtd = 1\n",
    "isFreezing = False\n",
    "batch_size = 10\n",
    "num_epoch = 200\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optim config\n",
    "learning_rate_bb = 0.02\n",
    "step_size_bb = 40\n",
    "gamma_bb = 0.5\n",
    "\n",
    "learning_rate_ds = 0.02\n",
    "step_size_ds = 20\n",
    "gamma_ds = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrega Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = HarDataModuleDownstram(batch_size=batch_size)\n",
    "train_dl, train_ds = data_module.get_dataloader(set=Sets.TRAIN.value, shuffle=True)\n",
    "test_dl, test_ds   = data_module.get_dataloader(set=Sets.TEST.value, shuffle=True)\n",
    "num_classes = len(train_ds.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ./best_models/backbone_motion.pth\n",
      "CNN1d(\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (backbone): Backbone(\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv1d(6, 12, kernel_size=(2,), stride=(1,))\n",
      "    (conv2): Conv1d(12, 24, kernel_size=(2,), stride=(1,))\n",
      "    (conv3): Conv1d(24, 48, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (pred_head): PredictionHead(\n",
      "    (linear1): Linear(in_features=288, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN1d(\n",
    "    data_label=main_data.value, \n",
    "    num_classes=num_classes, \n",
    "    require_grad= not isFreezing, \n",
    "    type=ModelTypes.DOWNSTREAM.value\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sr_rosa/.local/lib/python3.10/site-packages/lightning/pytorch/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/200], Step [   6/6], Loss: 1.7063 Validation Loss: 1.8147\n",
      "Epoch [   2/200], Step [   6/6], Loss: 1.8525 Validation Loss: 1.7750\n",
      "Epoch [   3/200], Step [   6/6], Loss: 1.7964 Validation Loss: 1.7231\n",
      "Epoch [   4/200], Step [   6/6], Loss: 1.7385 Validation Loss: 1.7411\n",
      "Epoch [   5/200], Step [   6/6], Loss: 1.7045 Validation Loss: 1.7050\n",
      "Epoch [   6/200], Step [   6/6], Loss: 1.6171 Validation Loss: 1.6825\n",
      "Epoch [   7/200], Step [   6/6], Loss: 1.8348 Validation Loss: 1.6748\n",
      "Epoch [   8/200], Step [   6/6], Loss: 1.7940 Validation Loss: 1.6772\n",
      "Epoch [   9/200], Step [   6/6], Loss: 1.6334 Validation Loss: 1.7316\n",
      "Epoch [  10/200], Step [   6/6], Loss: 1.6059 Validation Loss: 1.6074\n",
      "Epoch [  11/200], Step [   6/6], Loss: 1.6381 Validation Loss: 1.6728\n",
      "Epoch [  12/200], Step [   6/6], Loss: 1.6562 Validation Loss: 1.6016\n",
      "Epoch [  13/200], Step [   6/6], Loss: 1.7904 Validation Loss: 1.7400\n",
      "Epoch [  14/200], Step [   6/6], Loss: 1.5490 Validation Loss: 1.5375\n",
      "Epoch [  15/200], Step [   6/6], Loss: 1.2132 Validation Loss: 1.6046\n",
      "Epoch [  16/200], Step [   6/6], Loss: 3.6223 Validation Loss: 1.6244\n",
      "Epoch [  17/200], Step [   6/6], Loss: 1.6366 Validation Loss: 1.6994\n",
      "Epoch [  18/200], Step [   6/6], Loss: 1.6494 Validation Loss: 1.6248\n",
      "Epoch [  19/200], Step [   6/6], Loss: 1.5498 Validation Loss: 1.5862\n",
      "Epoch [  20/200], Step [   6/6], Loss: 1.7276 Validation Loss: 1.7258\n",
      "Epoch [  21/200], Step [   6/6], Loss: 1.8175 Validation Loss: 1.7440\n",
      "Epoch [  22/200], Step [   6/6], Loss: 1.3772 Validation Loss: 1.7257\n",
      "Epoch [  23/200], Step [   6/6], Loss: 1.7325 Validation Loss: 1.6722\n",
      "Epoch [  24/200], Step [   6/6], Loss: 1.5789 Validation Loss: 1.7376\n",
      "Epoch [  25/200], Step [   6/6], Loss: 1.7259 Validation Loss: 1.6827\n",
      "Epoch [  26/200], Step [   6/6], Loss: 1.7699 Validation Loss: 1.6525\n",
      "Epoch [  27/200], Step [   6/6], Loss: 1.6801 Validation Loss: 1.6035\n",
      "Epoch [  28/200], Step [   6/6], Loss: 1.6699 Validation Loss: 1.6756\n",
      "Epoch [  29/200], Step [   6/6], Loss: 1.9319 Validation Loss: 1.7479\n",
      "Epoch [  30/200], Step [   6/6], Loss: 1.8794 Validation Loss: 1.7064\n",
      "Epoch [  31/200], Step [   6/6], Loss: 1.7302 Validation Loss: 1.7032\n",
      "Epoch [  32/200], Step [   6/6], Loss: 1.5979 Validation Loss: 1.7493\n",
      "Epoch [  33/200], Step [   6/6], Loss: 1.8194 Validation Loss: 1.6428\n",
      "Epoch [  34/200], Step [   6/6], Loss: 1.8058 Validation Loss: 1.6899\n",
      "Epoch [  35/200], Step [   6/6], Loss: 1.8143 Validation Loss: 1.7225\n",
      "Epoch [  36/200], Step [   6/6], Loss: 1.6055 Validation Loss: 1.6949\n",
      "Epoch [  37/200], Step [   6/6], Loss: 1.6034 Validation Loss: 1.6272\n",
      "Epoch [  38/200], Step [   6/6], Loss: 1.9803 Validation Loss: 1.7119\n",
      "Epoch [  39/200], Step [   6/6], Loss: 1.9101 Validation Loss: 1.6701\n",
      "Epoch [  40/200], Step [   6/6], Loss: 1.5409 Validation Loss: 1.7158\n",
      "Epoch [  41/200], Step [   6/6], Loss: 1.6436 Validation Loss: 1.6293\n",
      "Epoch [  42/200], Step [   6/6], Loss: 1.5389 Validation Loss: 1.6549\n",
      "Epoch [  43/200], Step [   6/6], Loss: 1.7802 Validation Loss: 1.6755\n",
      "Epoch [  44/200], Step [   6/6], Loss: 1.6055 Validation Loss: 1.6786\n",
      "Epoch [  45/200], Step [   6/6], Loss: 1.6543 Validation Loss: 1.6338\n",
      "Epoch [  46/200], Step [   6/6], Loss: 1.7530 Validation Loss: 1.7156\n",
      "Epoch [  47/200], Step [   6/6], Loss: 1.5083 Validation Loss: 1.6417\n",
      "Epoch [  48/200], Step [   6/6], Loss: 1.7554 Validation Loss: 1.6854\n",
      "Epoch [  49/200], Step [   6/6], Loss: 1.6308 Validation Loss: 1.7202\n",
      "Epoch [  50/200], Step [   6/6], Loss: 1.5225 Validation Loss: 1.6896\n",
      "Epoch [  51/200], Step [   6/6], Loss: 1.8605 Validation Loss: 1.6593\n",
      "Epoch [  52/200], Step [   6/6], Loss: 1.7531 Validation Loss: 1.6930\n",
      "Epoch [  53/200], Step [   6/6], Loss: 1.6565 Validation Loss: 1.7229\n",
      "Epoch [  54/200], Step [   6/6], Loss: 1.7207 Validation Loss: 1.6472\n",
      "Epoch [  55/200], Step [   6/6], Loss: 1.6224 Validation Loss: 1.6859\n",
      "Epoch [  56/200], Step [   6/6], Loss: 1.7019 Validation Loss: 1.6358\n",
      "Epoch [  57/200], Step [   6/6], Loss: 1.7152 Validation Loss: 1.6334\n",
      "Epoch [  58/200], Step [   6/6], Loss: 1.4806 Validation Loss: 1.6111\n",
      "Epoch [  59/200], Step [   6/6], Loss: 1.5806 Validation Loss: 1.6353\n",
      "Epoch [  60/200], Step [   6/6], Loss: 1.5539 Validation Loss: 1.5990\n",
      "Epoch [  61/200], Step [   6/6], Loss: 1.3938 Validation Loss: 1.6401\n",
      "Epoch [  62/200], Step [   6/6], Loss: 1.6087 Validation Loss: 1.6192\n",
      "Epoch [  63/200], Step [   6/6], Loss: 1.8732 Validation Loss: 1.5644\n",
      "Epoch [  64/200], Step [   6/6], Loss: 1.5622 Validation Loss: 1.6019\n",
      "Epoch [  65/200], Step [   6/6], Loss: 2.0119 Validation Loss: 1.6503\n",
      "Epoch [  66/200], Step [   6/6], Loss: 1.4718 Validation Loss: 1.4988\n",
      "Epoch [  67/200], Step [   6/6], Loss: 1.7386 Validation Loss: 1.6459\n",
      "Epoch [  68/200], Step [   6/6], Loss: 1.5174 Validation Loss: 1.6756\n",
      "Epoch [  69/200], Step [   6/6], Loss: 1.3560 Validation Loss: 1.6408\n",
      "Epoch [  70/200], Step [   6/6], Loss: 1.7777 Validation Loss: 1.6407\n",
      "Epoch [  71/200], Step [   6/6], Loss: 1.6007 Validation Loss: 1.5453\n",
      "Epoch [  72/200], Step [   6/6], Loss: 1.4469 Validation Loss: 1.6363\n",
      "Epoch [  73/200], Step [   6/6], Loss: 1.5770 Validation Loss: 1.6126\n",
      "Epoch [  74/200], Step [   6/6], Loss: 1.4785 Validation Loss: 1.5984\n",
      "Epoch [  75/200], Step [   6/6], Loss: 1.7076 Validation Loss: 1.5687\n",
      "Epoch [  76/200], Step [   6/6], Loss: 1.5760 Validation Loss: 1.5491\n",
      "Epoch [  77/200], Step [   6/6], Loss: 1.6447 Validation Loss: 1.5651\n",
      "Epoch [  78/200], Step [   6/6], Loss: 1.7615 Validation Loss: 1.6307\n",
      "Epoch [  79/200], Step [   6/6], Loss: 1.5169 Validation Loss: 1.6454\n",
      "Epoch [  80/200], Step [   6/6], Loss: 1.6613 Validation Loss: 1.5397\n",
      "Epoch [  81/200], Step [   6/6], Loss: 1.6725 Validation Loss: 1.6262\n",
      "Epoch [  82/200], Step [   6/6], Loss: 1.6564 Validation Loss: 1.6155\n",
      "Epoch [  83/200], Step [   6/6], Loss: 1.7495 Validation Loss: 1.5963\n",
      "Epoch [  84/200], Step [   6/6], Loss: 1.5834 Validation Loss: 1.5312\n",
      "Epoch [  85/200], Step [   6/6], Loss: 1.8428 Validation Loss: 1.5531\n",
      "Epoch [  86/200], Step [   6/6], Loss: 1.5782 Validation Loss: 1.6012\n",
      "Epoch [  87/200], Step [   6/6], Loss: 1.6103 Validation Loss: 1.5613\n",
      "Epoch [  88/200], Step [   6/6], Loss: 1.6157 Validation Loss: 1.5473\n",
      "Epoch [  89/200], Step [   6/6], Loss: 1.6752 Validation Loss: 1.5357\n",
      "Epoch [  90/200], Step [   6/6], Loss: 1.5939 Validation Loss: 1.5669\n",
      "Epoch [  91/200], Step [   6/6], Loss: 1.4790 Validation Loss: 1.6342\n",
      "Epoch [  92/200], Step [   6/6], Loss: 1.6993 Validation Loss: 1.4891\n",
      "Epoch [  93/200], Step [   6/6], Loss: 1.5988 Validation Loss: 1.4854\n",
      "Epoch [  94/200], Step [   6/6], Loss: 1.4305 Validation Loss: 1.5477\n",
      "Epoch [  95/200], Step [   6/6], Loss: 1.6079 Validation Loss: 1.4436\n",
      "Epoch [  96/200], Step [   6/6], Loss: 1.3191 Validation Loss: 1.5623\n",
      "Epoch [  97/200], Step [   6/6], Loss: 1.6962 Validation Loss: 1.5837\n",
      "Epoch [  98/200], Step [   6/6], Loss: 1.8454 Validation Loss: 1.5664\n",
      "Epoch [  99/200], Step [   6/6], Loss: 1.3573 Validation Loss: 1.5556\n",
      "Epoch [ 100/200], Step [   6/6], Loss: 1.3555 Validation Loss: 1.5520\n",
      "Epoch [ 101/200], Step [   6/6], Loss: 1.3289 Validation Loss: 1.5365\n",
      "Epoch [ 102/200], Step [   6/6], Loss: 1.3583 Validation Loss: 1.6218\n",
      "Epoch [ 103/200], Step [   6/6], Loss: 1.4504 Validation Loss: 1.4087\n",
      "Epoch [ 104/200], Step [   6/6], Loss: 1.1280 Validation Loss: 1.4995\n",
      "Epoch [ 105/200], Step [   6/6], Loss: 0.9004 Validation Loss: 1.5646\n",
      "Epoch [ 106/200], Step [   6/6], Loss: 1.7654 Validation Loss: 1.4950\n",
      "Epoch [ 107/200], Step [   6/6], Loss: 1.3308 Validation Loss: 1.5729\n",
      "Epoch [ 108/200], Step [   6/6], Loss: 1.4289 Validation Loss: 1.8156\n",
      "Epoch [ 109/200], Step [   6/6], Loss: 1.5373 Validation Loss: 1.5639\n",
      "Epoch [ 110/200], Step [   6/6], Loss: 1.2343 Validation Loss: 1.6478\n",
      "Epoch [ 111/200], Step [   6/6], Loss: 1.4385 Validation Loss: 1.5389\n",
      "Epoch [ 112/200], Step [   6/6], Loss: 1.2711 Validation Loss: 1.5822\n",
      "Epoch [ 113/200], Step [   6/6], Loss: 1.1495 Validation Loss: 1.5223\n",
      "Epoch [ 114/200], Step [   6/6], Loss: 1.2672 Validation Loss: 1.7136\n",
      "Epoch [ 115/200], Step [   6/6], Loss: 1.4395 Validation Loss: 1.6732\n",
      "Epoch [ 116/200], Step [   6/6], Loss: 1.3015 Validation Loss: 1.6112\n",
      "Epoch [ 117/200], Step [   6/6], Loss: 1.7551 Validation Loss: 1.4213\n",
      "Epoch [ 118/200], Step [   6/6], Loss: 1.4896 Validation Loss: 1.4961\n",
      "Epoch [ 119/200], Step [   6/6], Loss: 1.2961 Validation Loss: 1.6095\n",
      "Epoch [ 120/200], Step [   6/6], Loss: 1.3656 Validation Loss: 1.8749\n",
      "Epoch [ 121/200], Step [   6/6], Loss: 1.2907 Validation Loss: 1.6751\n",
      "Epoch [ 122/200], Step [   6/6], Loss: 1.5454 Validation Loss: 1.6087\n",
      "Epoch [ 123/200], Step [   6/6], Loss: 1.5225 Validation Loss: 1.5738\n",
      "Epoch [ 124/200], Step [   6/6], Loss: 1.4737 Validation Loss: 1.6649\n",
      "Epoch [ 125/200], Step [   6/6], Loss: 1.4915 Validation Loss: 1.5341\n",
      "Epoch [ 126/200], Step [   6/6], Loss: 0.9661 Validation Loss: 1.5764\n",
      "Epoch [ 127/200], Step [   6/6], Loss: 1.1069 Validation Loss: 1.5766\n",
      "Epoch [ 128/200], Step [   6/6], Loss: 1.6597 Validation Loss: 1.6529\n",
      "Epoch [ 129/200], Step [   6/6], Loss: 1.1532 Validation Loss: 1.5722\n",
      "Epoch [ 130/200], Step [   6/6], Loss: 1.6358 Validation Loss: 1.7132\n",
      "Epoch [ 131/200], Step [   6/6], Loss: 1.3996 Validation Loss: 1.5005\n",
      "Epoch [ 132/200], Step [   6/6], Loss: 1.7435 Validation Loss: 1.6783\n",
      "Epoch [ 133/200], Step [   6/6], Loss: 1.0684 Validation Loss: 1.6561\n",
      "Epoch [ 134/200], Step [   6/6], Loss: 1.2186 Validation Loss: 1.6717\n",
      "Epoch [ 135/200], Step [   6/6], Loss: 1.1829 Validation Loss: 1.6598\n",
      "Epoch [ 136/200], Step [   6/6], Loss: 1.4017 Validation Loss: 1.8383\n",
      "Epoch [ 137/200], Step [   6/6], Loss: 1.0178 Validation Loss: 1.6831\n",
      "Epoch [ 138/200], Step [   6/6], Loss: 1.3550 Validation Loss: 1.5300\n",
      "Epoch [ 139/200], Step [   6/6], Loss: 1.2207 Validation Loss: 1.7621\n",
      "Epoch [ 140/200], Step [   6/6], Loss: 1.4803 Validation Loss: 1.4742\n",
      "Epoch [ 141/200], Step [   6/6], Loss: 1.3976 Validation Loss: 1.5895\n",
      "Epoch [ 142/200], Step [   6/6], Loss: 1.6411 Validation Loss: 1.8677\n",
      "Epoch [ 143/200], Step [   6/6], Loss: 1.1944 Validation Loss: 1.6685\n",
      "Epoch [ 144/200], Step [   6/6], Loss: 1.0648 Validation Loss: 1.6816\n",
      "Epoch [ 145/200], Step [   6/6], Loss: 1.2633 Validation Loss: 1.9489\n",
      "Epoch [ 146/200], Step [   6/6], Loss: 1.6110 Validation Loss: 1.6215\n",
      "Epoch [ 147/200], Step [   6/6], Loss: 1.3233 Validation Loss: 1.7478\n",
      "Epoch [ 148/200], Step [   6/6], Loss: 1.3450 Validation Loss: 1.4278\n",
      "Epoch [ 149/200], Step [   6/6], Loss: 1.5770 Validation Loss: 1.4896\n",
      "Epoch [ 150/200], Step [   6/6], Loss: 1.2027 Validation Loss: 1.5047\n",
      "Epoch [ 151/200], Step [   6/6], Loss: 1.2488 Validation Loss: 1.5660\n",
      "Epoch [ 152/200], Step [   6/6], Loss: 1.6513 Validation Loss: 1.6044\n",
      "Epoch [ 153/200], Step [   6/6], Loss: 1.2155 Validation Loss: 1.7564\n",
      "Epoch [ 154/200], Step [   6/6], Loss: 1.0313 Validation Loss: 1.5482\n",
      "Epoch [ 155/200], Step [   6/6], Loss: 1.4220 Validation Loss: 1.6305\n",
      "Epoch [ 156/200], Step [   6/6], Loss: 1.3660 Validation Loss: 1.6787\n",
      "Epoch [ 157/200], Step [   6/6], Loss: 1.4417 Validation Loss: 1.7620\n",
      "Epoch [ 158/200], Step [   6/6], Loss: 1.4567 Validation Loss: 1.7644\n",
      "Epoch [ 159/200], Step [   6/6], Loss: 1.2243 Validation Loss: 1.9751\n",
      "Epoch [ 160/200], Step [   6/6], Loss: 1.4461 Validation Loss: 1.6130\n",
      "Epoch [ 161/200], Step [   6/6], Loss: 1.6124 Validation Loss: 1.6043\n",
      "Epoch [ 162/200], Step [   6/6], Loss: 1.4650 Validation Loss: 1.6854\n",
      "Epoch [ 163/200], Step [   6/6], Loss: 1.6856 Validation Loss: 1.9320\n",
      "Epoch [ 164/200], Step [   6/6], Loss: 1.5495 Validation Loss: 1.6623\n",
      "Epoch [ 165/200], Step [   6/6], Loss: 1.2663 Validation Loss: 1.5643\n",
      "Epoch [ 166/200], Step [   6/6], Loss: 1.1251 Validation Loss: 1.6563\n",
      "Epoch [ 167/200], Step [   6/6], Loss: 1.5905 Validation Loss: 1.6841\n",
      "Epoch [ 168/200], Step [   6/6], Loss: 1.4199 Validation Loss: 1.5136\n",
      "Epoch [ 169/200], Step [   6/6], Loss: 1.0053 Validation Loss: 1.6076\n",
      "Epoch [ 170/200], Step [   6/6], Loss: 1.6250 Validation Loss: 1.5551\n",
      "Epoch [ 171/200], Step [   6/6], Loss: 1.4426 Validation Loss: 1.8810\n",
      "Epoch [ 172/200], Step [   6/6], Loss: 1.4597 Validation Loss: 1.6236\n",
      "Epoch [ 173/200], Step [   6/6], Loss: 1.3879 Validation Loss: 1.5478\n",
      "Epoch [ 174/200], Step [   6/6], Loss: 1.1502 Validation Loss: 1.8677\n",
      "Epoch [ 175/200], Step [   6/6], Loss: 1.3775 Validation Loss: 1.5307\n",
      "Epoch [ 176/200], Step [   6/6], Loss: 1.3988 Validation Loss: 1.5456\n",
      "Epoch [ 177/200], Step [   6/6], Loss: 1.5004 Validation Loss: 1.6834\n",
      "Epoch [ 178/200], Step [   6/6], Loss: 1.5010 Validation Loss: 1.6939\n",
      "Epoch [ 179/200], Step [   6/6], Loss: 1.3594 Validation Loss: 1.6124\n",
      "Epoch [ 180/200], Step [   6/6], Loss: 1.8354 Validation Loss: 1.6029\n",
      "Epoch [ 181/200], Step [   6/6], Loss: 1.8199 Validation Loss: 1.6677\n",
      "Epoch [ 182/200], Step [   6/6], Loss: 1.2712 Validation Loss: 1.6212\n",
      "Epoch [ 183/200], Step [   6/6], Loss: 1.3628 Validation Loss: 1.5894\n",
      "Epoch [ 184/200], Step [   6/6], Loss: 1.3531 Validation Loss: 1.6470\n",
      "Epoch [ 185/200], Step [   6/6], Loss: 1.1762 Validation Loss: 1.4973\n",
      "Epoch [ 186/200], Step [   6/6], Loss: 1.2490 Validation Loss: 1.8534\n",
      "Epoch [ 187/200], Step [   6/6], Loss: 1.7164 Validation Loss: 1.5205\n",
      "Epoch [ 188/200], Step [   6/6], Loss: 1.3881 Validation Loss: 1.6923\n",
      "Epoch [ 189/200], Step [   6/6], Loss: 1.5007 Validation Loss: 1.5940\n",
      "Epoch [ 190/200], Step [   6/6], Loss: 1.1554 Validation Loss: 1.5365\n",
      "Epoch [ 191/200], Step [   6/6], Loss: 1.5860 Validation Loss: 1.6482\n",
      "Epoch [ 192/200], Step [   6/6], Loss: 1.0890 Validation Loss: 1.8178\n",
      "Epoch [ 193/200], Step [   6/6], Loss: 1.6499 Validation Loss: 1.5986\n",
      "Epoch [ 194/200], Step [   6/6], Loss: 1.7221 Validation Loss: 1.5091\n",
      "Epoch [ 195/200], Step [   6/6], Loss: 0.7398 Validation Loss: 1.7836\n",
      "Epoch [ 196/200], Step [   6/6], Loss: 1.4240 Validation Loss: 1.9350\n",
      "Epoch [ 197/200], Step [   6/6], Loss: 1.2385 Validation Loss: 1.6550\n",
      "Epoch [ 198/200], Step [   6/6], Loss: 1.2052 Validation Loss: 1.7450\n",
      "Epoch [ 199/200], Step [   6/6], Loss: 1.0023 Validation Loss: 1.5574\n",
      "Epoch [ 200/200], Step [   6/6], Loss: 1.4816 Validation Loss: 1.6204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import lightning as L\n",
    "# trainer = L.Trainer(\n",
    "#     max_epochs=10,\n",
    "#     accelerator='cpu',\n",
    "#     log_every_n_steps=1        \n",
    "# )\n",
    "# trainer.fit(model=model, train_dataloaders=train_dl)\n",
    "\n",
    "optimizer_backbone, lr_scheduler_backbone = model.configure_backbone_optimizers(step_size=step_size_bb, gamma=gamma_bb, learning_rate=learning_rate_bb)\n",
    "optimizer_downstream, lr_scheduler_downstream = model.configure_head_optimizers(step_size=step_size_ds, gamma=gamma_ds, learning_rate=learning_rate_ds)\n",
    "\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "best_val_loss = 500\n",
    "n_total_steps = len(train_dl)\n",
    "for epoch in range(num_epoch):\n",
    "    # Treinamento\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        loss = model.training_step(batch)\n",
    "        train_loss += loss.item()\n",
    "        optimizer_backbone.zero_grad()\n",
    "        optimizer_downstream.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_backbone.step()\n",
    "        optimizer_downstream.step()\n",
    "        \n",
    "        if (i+1) % math.floor(n_total_steps/printQtd) == 0:\n",
    "            print (f'Epoch [{epoch+1:4d}/{num_epoch}], Step [{i+1:4d}/{n_total_steps}], Loss: {loss.item():.4f}', end= \"\" if n_total_steps/printQtd+i >= n_total_steps else \"\\n\")\n",
    "\n",
    "    lr_scheduler_backbone.step()\n",
    "    lr_scheduler_downstream.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            val_loss += model.validation_step(batch)\n",
    "    \n",
    "    val_loss /= len(test_dl)\n",
    "    print(f' Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    train_errors.append(train_loss/len(train_dl))\n",
    "    validation_errors.append(val_loss.item())\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict()  # Salva os parâmetros do modelo\n",
    "\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 33.333333333333336 %\n",
      "Accuracy of Run (4/4 | 15): 100.0 %\n",
      "Accuracy of Sit (4/4 | 8): 100.0 %\n",
      "Accuracy of Stair-down (0/4 | 0): 0.0 %\n",
      "Accuracy of Stair-up (0/4 | 1): 0.0 %\n",
      "Accuracy of Stand (0/4 | 0): 0.0 %\n",
      "Accuracy of Walk (0/4 | 0): 0.0 %\n"
     ]
    }
   ],
   "source": [
    "accTotal = 0\n",
    "predicted_values = []\n",
    "real_values = []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(num_classes)]\n",
    "    n_class_samples = [0 for i in range(num_classes)]\n",
    "    n_each_class_samples = [0 for i in range(num_classes)]\n",
    "\n",
    "    for data, labels in test_dl:\n",
    "        outputs = model(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for pred, real in zip (predicted, labels):\n",
    "            predicted_values.append(pred.item())\n",
    "            real_values.append(real.item())\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            label = labels[i]\n",
    "            pred  = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            n_each_class_samples[pred] += 1\n",
    "\n",
    "    accTotal = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {accTotal} %')\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {test_ds.getLabel(i)} ({n_class_correct[i]}/{n_class_samples[i]} | {n_each_class_samples[i]}): {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salva Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_full_model(accuracy=accTotal, batch_size=batch_size, num_epoch=num_epoch)\n",
    "\n",
    "pred_reports = pd.DataFrame({\n",
    "    Sets.REAL.value: real_values,\n",
    "    Sets.PREDICTION.value : predicted_values\n",
    "})\n",
    "pred_reports.to_csv(f\"{path_reports}/predictions_{ModelTypes.DOWNSTREAM.value}.dat\", sep=\" \", index=False)\n",
    "\n",
    "train_reports = pd.DataFrame({\n",
    "    Sets.TRAIN.value : train_errors,\n",
    "    Sets.VALIDATION.value : validation_errors\n",
    "})\n",
    "train_reports.to_csv(f\"{path_reports}/errors_{ModelTypes.DOWNSTREAM.value}.dat\", sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realiza treinamento com percentuais da base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperparâmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_modules.har import HarDataModule as HarDataModuleDownstram\n",
    "from utils.enums import Datas, Sets, ModelTypes\n",
    "from models.cnn1d import CNN1d\n",
    "\n",
    "printQtd = 1\n",
    "isFreezing = False\n",
    "batch_size = 10\n",
    "num_epoch = 200\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optim config\n",
    "learning_rate = 0.02\n",
    "step_size = 50\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realiza Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ./best_models/backbone_motion.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sr_rosa/.local/lib/python3.10/site-packages/lightning/pytorch/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     44\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mtraining_step(batch)\n\u001b[0;32m---> 45\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m optimizer1\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m optimizer2\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/ssl/models/cnn1d.py:111\u001b[0m, in \u001b[0;36mCNN1d.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ssl/models/cnn1d.py:99\u001b[0m, in \u001b[0;36mCNN1d._common_step\u001b[0;34m(self, batch, set)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_common_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mset\u001b[39m:Sets):\n\u001b[1;32m     98\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 99\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred, y)\n\u001b[1;32m    102\u001b[0m     class_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/ssl/models/cnn1d.py:93\u001b[0m, in \u001b[0;36mCNN1d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_head(x)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ssl/models/cnn1d.py:31\u001b[0m, in \u001b[0;36mBackbone.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, negative_slope\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x), negative_slope\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "percents = [1, 20, 40, 60, 80, 100]\n",
    "\n",
    "for percent in percents:\n",
    "\n",
    "    # Carrega a base de dados\n",
    "    data_module = HarDataModuleDownstram(batch_size=batch_size)\n",
    "    train_dl, train_ds = data_module.get_dataloader(set=Sets.TRAIN.value, shuffle=True, percent=percent/100)\n",
    "    test_dl, test_ds   = data_module.get_dataloader(set=Sets.TEST.value, shuffle=False)\n",
    "    num_classes = len(test_ds.labels)\n",
    "\n",
    "    # Define os modelos\n",
    "    model1 = CNN1d(\n",
    "        data_label=main_data.value, \n",
    "        num_classes=num_classes, \n",
    "        require_grad= not isFreezing, \n",
    "        type=ModelTypes.DOWNSTREAM.value\n",
    "    )\n",
    "    model2 = CNN1d(\n",
    "        data_label=main_data.value, \n",
    "        num_classes=num_classes, \n",
    "        require_grad=True, \n",
    "        type=ModelTypes.PRETEXT.value\n",
    "    )\n",
    "\n",
    "    # Define os otimizadores\n",
    "    optimizer1, lr_scheduler1 = model1.configure_optimizers(step_size=step_size, gamma=gamma, learning_rate=learning_rate)\n",
    "    optimizer2, lr_scheduler2 = model2.configure_optimizers(step_size=step_size, gamma=gamma, learning_rate=learning_rate)\n",
    "\n",
    "    optimizer1, lr_scheduler1 = optimizer1[0], lr_scheduler1[0]\n",
    "    optimizer2, lr_scheduler2 = optimizer2[0], lr_scheduler2[0]\n",
    "\n",
    "    # Realiza o treinamento\n",
    "    best_val_loss1 = 500\n",
    "    best_val_loss2 = 500\n",
    "    n_total_steps = len(train_dl)\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        # Treinamento \n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        for i, batch in enumerate(train_dl):\n",
    "            if len(batch) <= 0:\n",
    "                break\n",
    "            loss1 = model1.training_step(batch)\n",
    "            loss2 = model2.training_step(batch)\n",
    "\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "            loss1.backward()\n",
    "            loss2.backward()\n",
    "\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "\n",
    "        lr_scheduler1.step()\n",
    "        lr_scheduler2.step()\n",
    "\n",
    "        # Validação\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "\n",
    "        val_loss1 = 0\n",
    "        val_loss2 = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dl:\n",
    "                val_loss1 += model1.validation_step(batch)\n",
    "                val_loss2 += model2.validation_step(batch)\n",
    "        \n",
    "        val_loss1 /= len(test_dl)\n",
    "        val_loss2 /= len(test_dl)\n",
    "\n",
    "        if val_loss1 < best_val_loss1:\n",
    "            best_val_loss1 = val_loss1\n",
    "            best_model1 = model1.state_dict() \n",
    "\n",
    "        if val_loss2 < best_val_loss2:\n",
    "            best_val_loss2 = val_loss2\n",
    "            best_model2 = model2.state_dict()  \n",
    "\n",
    "    model1.load_state_dict(best_model1)\n",
    "    model2.load_state_dict(best_model2)\n",
    "\n",
    "    predicted_values1 = []\n",
    "    predicted_values2 = []\n",
    "\n",
    "    real_values1 = []\n",
    "    real_values2 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dl:\n",
    "            outputs1 = model1(data)\n",
    "            _, predicted1 = torch.max(outputs1, 1)\n",
    "\n",
    "            for pred, real in zip (predicted1, labels):\n",
    "                predicted_values1.append(pred.item())\n",
    "                real_values1.append(real.item())\n",
    "\n",
    "            outputs2 = model2(data)\n",
    "            _, predicted2 = torch.max(outputs2, 1)\n",
    "\n",
    "            for pred, real in zip (predicted2, labels):\n",
    "                predicted_values2.append(pred.item())\n",
    "                real_values2.append(real.item())\n",
    "\n",
    "    pred_reports1 = pd.DataFrame({\n",
    "        Sets.REAL.value: real_values1,\n",
    "        Sets.PREDICTION.value : predicted_values1\n",
    "    })\n",
    "    pred_reports2 = pd.DataFrame({\n",
    "        Sets.REAL.value: real_values2,\n",
    "        Sets.PREDICTION.value : predicted_values2\n",
    "    })\n",
    "\n",
    "    pred_reports1.to_csv(f\"report_results/har/percent/cnn/predictions_{percent}_m1.dat\", sep=\" \", index=False)\n",
    "    pred_reports2.to_csv(f\"report_results/har/percent/cnn/predictions_{percent}_m2.dat\", sep=\" \", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

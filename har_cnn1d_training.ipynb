{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria repositório de reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.enums import Datas\n",
    "from utils.checkpoints import verifyPath\n",
    "\n",
    "teste_size = 2\n",
    "main_data = Datas.MOTION\n",
    "path_reports = f\"report_results/{Datas.HAR.value}/{main_data.value}_{teste_size}/\"\n",
    "\n",
    "split_path = path_reports.split(\"/\")\n",
    "partial_path = \"\"\n",
    "for i, part in enumerate(split_path):\n",
    "    partial_path += part + \"/\"\n",
    "    verifyPath(partial_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treina Pretexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Hiperparâmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from models.cnn1d import CNN1d\n",
    "from utils.enums import Datas, Sets, ModelTypes\n",
    "from data_modules.pretext import HarDataModule as HarDataModulePretext\n",
    "from transforms.har import rotation, flip, noise_addition, permutation, scaling, time_warp, negation\n",
    "\n",
    "printQtd = 1\n",
    "num_epoch = 320\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optim\n",
    "learning_rate = 0.02\n",
    "step_size = 80\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrega Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [rotation, flip, noise_addition, permutation, scaling, time_warp, negation]\n",
    "data_module = HarDataModulePretext(batch_size=batch_size, main_data = main_data)\n",
    "train_dl, train_ds = data_module.get_dataloader(set=Sets.TRAIN.value, shuffle=True, transforms=transforms)\n",
    "test_dl, test_ds   = data_module.get_dataloader(set=Sets.TEST.value, shuffle=False, transforms=transforms)\n",
    "num_classes = len(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1d(\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (backbone): Backbone(\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv1d(6, 12, kernel_size=(2,), stride=(1,))\n",
      "    (conv2): Conv1d(12, 24, kernel_size=(2,), stride=(1,))\n",
      "    (conv3): Conv1d(24, 48, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (pred_head): ProjectionHead(\n",
      "    (linear1): Linear(in_features=288, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_p = CNN1d(data_label=main_data.value, num_classes=num_classes, require_grad=True, type=ModelTypes.PRETEXT.value)\n",
    "print(model_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sr_rosa/.local/lib/python3.10/site-packages/lightning/pytorch/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   1/320], Step [ 104/104], Loss: 1.9414 Validation Loss: 1.9451\n",
      "Epoch [   2/320], Step [ 104/104], Loss: 1.9578 Validation Loss: 1.9464\n",
      "Epoch [   3/320], Step [ 104/104], Loss: 1.9331 Validation Loss: 1.9434\n",
      "Epoch [   4/320], Step [ 104/104], Loss: 1.9609 Validation Loss: 1.9438\n",
      "Epoch [   5/320], Step [ 104/104], Loss: 1.9477 Validation Loss: 1.9412\n",
      "Epoch [   6/320], Step [ 104/104], Loss: 1.9508 Validation Loss: 1.9403\n",
      "Epoch [   7/320], Step [ 104/104], Loss: 1.9353 Validation Loss: 1.9382\n",
      "Epoch [   8/320], Step [ 104/104], Loss: 1.9671 Validation Loss: 1.9408\n",
      "Epoch [   9/320], Step [ 104/104], Loss: 1.9364 Validation Loss: 1.9338\n",
      "Epoch [  10/320], Step [ 104/104], Loss: 1.9212 Validation Loss: 1.9293\n",
      "Epoch [  11/320], Step [ 104/104], Loss: 1.9571 Validation Loss: 1.9356\n",
      "Epoch [  12/320], Step [ 104/104], Loss: 1.9127 Validation Loss: 1.9311\n",
      "Epoch [  13/320], Step [ 104/104], Loss: 1.9036 Validation Loss: 1.9250\n",
      "Epoch [  14/320], Step [ 104/104], Loss: 1.9047 Validation Loss: 1.9217\n",
      "Epoch [  15/320], Step [ 104/104], Loss: 1.9438 Validation Loss: 1.8938\n",
      "Epoch [  16/320], Step [ 104/104], Loss: 1.8590 Validation Loss: 1.8763\n",
      "Epoch [  17/320], Step [ 104/104], Loss: 1.8523 Validation Loss: 1.8610\n",
      "Epoch [  18/320], Step [ 104/104], Loss: 1.9065 Validation Loss: 1.8543\n",
      "Epoch [  19/320], Step [ 104/104], Loss: 1.7759 Validation Loss: 1.8381\n",
      "Epoch [  20/320], Step [ 104/104], Loss: 1.8844 Validation Loss: 1.8259\n",
      "Epoch [  21/320], Step [ 104/104], Loss: 1.9318 Validation Loss: 1.8110\n",
      "Epoch [  22/320], Step [ 104/104], Loss: 1.8245 Validation Loss: 1.7890\n",
      "Epoch [  23/320], Step [ 104/104], Loss: 1.7747 Validation Loss: 1.7760\n",
      "Epoch [  24/320], Step [ 104/104], Loss: 1.8220 Validation Loss: 1.7685\n",
      "Epoch [  25/320], Step [ 104/104], Loss: 1.9273 Validation Loss: 1.7404\n",
      "Epoch [  26/320], Step [ 104/104], Loss: 1.6857 Validation Loss: 1.6818\n",
      "Epoch [  27/320], Step [ 104/104], Loss: 1.6984 Validation Loss: 1.6931\n",
      "Epoch [  28/320], Step [ 104/104], Loss: 1.6254 Validation Loss: 1.6631\n",
      "Epoch [  29/320], Step [ 104/104], Loss: 1.6449 Validation Loss: 1.5934\n",
      "Epoch [  30/320], Step [ 104/104], Loss: 1.6392 Validation Loss: 1.6363\n",
      "Epoch [  31/320], Step [ 104/104], Loss: 1.6145 Validation Loss: 1.6102\n",
      "Epoch [  32/320], Step [ 104/104], Loss: 1.3192 Validation Loss: 1.6241\n",
      "Epoch [  33/320], Step [ 104/104], Loss: 1.6880 Validation Loss: 1.5819\n",
      "Epoch [  34/320], Step [ 104/104], Loss: 1.3809 Validation Loss: 1.5423\n",
      "Epoch [  35/320], Step [ 104/104], Loss: 1.6636 Validation Loss: 1.5574\n",
      "Epoch [  36/320], Step [ 104/104], Loss: 1.4282 Validation Loss: 1.5361\n",
      "Epoch [  37/320], Step [ 104/104], Loss: 1.6135 Validation Loss: 1.5277\n",
      "Epoch [  38/320], Step [ 104/104], Loss: 1.7424 Validation Loss: 1.5083\n",
      "Epoch [  39/320], Step [ 104/104], Loss: 1.3675 Validation Loss: 1.4574\n",
      "Epoch [  40/320], Step [ 104/104], Loss: 1.1325 Validation Loss: 1.4608\n",
      "Epoch [  41/320], Step [ 104/104], Loss: 1.5074 Validation Loss: 1.6376\n",
      "Epoch [  42/320], Step [ 104/104], Loss: 1.5469 Validation Loss: 1.4678\n",
      "Epoch [  43/320], Step [ 104/104], Loss: 1.3698 Validation Loss: 1.4605\n",
      "Epoch [  44/320], Step [ 104/104], Loss: 1.1359 Validation Loss: 1.4395\n",
      "Epoch [  45/320], Step [ 104/104], Loss: 1.4578 Validation Loss: 1.3891\n",
      "Epoch [  46/320], Step [ 104/104], Loss: 1.3689 Validation Loss: 1.3919\n",
      "Epoch [  47/320], Step [ 104/104], Loss: 1.0285 Validation Loss: 1.4101\n",
      "Epoch [  48/320], Step [ 104/104], Loss: 1.5572 Validation Loss: 1.5357\n",
      "Epoch [  49/320], Step [ 104/104], Loss: 1.7280 Validation Loss: 1.3060\n",
      "Epoch [  50/320], Step [ 104/104], Loss: 1.4262 Validation Loss: 1.2843\n",
      "Epoch [  51/320], Step [ 104/104], Loss: 1.3875 Validation Loss: 1.3235\n",
      "Epoch [  52/320], Step [ 104/104], Loss: 1.2829 Validation Loss: 1.4161\n",
      "Epoch [  53/320], Step [ 104/104], Loss: 1.4892 Validation Loss: 1.3685\n",
      "Epoch [  54/320], Step [ 104/104], Loss: 1.4790 Validation Loss: 1.3486\n",
      "Epoch [  55/320], Step [ 104/104], Loss: 1.2319 Validation Loss: 1.3104\n",
      "Epoch [  56/320], Step [ 104/104], Loss: 1.1962 Validation Loss: 1.2323\n",
      "Epoch [  57/320], Step [ 104/104], Loss: 0.9457 Validation Loss: 1.2895\n",
      "Epoch [  58/320], Step [ 104/104], Loss: 1.0127 Validation Loss: 1.2953\n",
      "Epoch [  59/320], Step [ 104/104], Loss: 1.1864 Validation Loss: 1.2855\n",
      "Epoch [  60/320], Step [ 104/104], Loss: 1.0254 Validation Loss: 1.2387\n",
      "Epoch [  61/320], Step [ 104/104], Loss: 1.2413 Validation Loss: 1.2205\n",
      "Epoch [  62/320], Step [ 104/104], Loss: 1.3744 Validation Loss: 1.3789\n",
      "Epoch [  63/320], Step [ 104/104], Loss: 0.9460 Validation Loss: 1.2012\n",
      "Epoch [  64/320], Step [ 104/104], Loss: 1.0656 Validation Loss: 1.2803\n",
      "Epoch [  65/320], Step [ 104/104], Loss: 1.3062 Validation Loss: 1.1900\n",
      "Epoch [  66/320], Step [ 104/104], Loss: 1.4475 Validation Loss: 1.5206\n",
      "Epoch [  67/320], Step [ 104/104], Loss: 1.1219 Validation Loss: 1.1438\n",
      "Epoch [  68/320], Step [ 104/104], Loss: 1.1325 Validation Loss: 1.2512\n",
      "Epoch [  69/320], Step [ 104/104], Loss: 1.1795 Validation Loss: 1.1742\n",
      "Epoch [  70/320], Step [ 104/104], Loss: 0.5796 Validation Loss: 1.1964\n",
      "Epoch [  71/320], Step [ 104/104], Loss: 1.1008 Validation Loss: 1.0922\n",
      "Epoch [  72/320], Step [ 104/104], Loss: 1.1476 Validation Loss: 1.1681\n",
      "Epoch [  73/320], Step [ 104/104], Loss: 0.9766 Validation Loss: 1.1846\n",
      "Epoch [  74/320], Step [ 104/104], Loss: 1.3684 Validation Loss: 1.0455\n",
      "Epoch [  75/320], Step [ 104/104], Loss: 0.9541 Validation Loss: 1.1001\n",
      "Epoch [  76/320], Step [ 104/104], Loss: 0.9965 Validation Loss: 1.1141\n",
      "Epoch [  77/320], Step [ 104/104], Loss: 1.1428 Validation Loss: 1.0838\n",
      "Epoch [  78/320], Step [ 104/104], Loss: 1.5380 Validation Loss: 1.0951\n",
      "Epoch [  79/320], Step [ 104/104], Loss: 1.3323 Validation Loss: 1.0938\n",
      "Epoch [  80/320], Step [ 104/104], Loss: 0.9575 Validation Loss: 1.0613\n",
      "Epoch [  81/320], Step [ 104/104], Loss: 1.1504 Validation Loss: 1.0592\n",
      "Epoch [  82/320], Step [ 104/104], Loss: 1.2686 Validation Loss: 1.0358\n",
      "Epoch [  83/320], Step [ 104/104], Loss: 0.8623 Validation Loss: 1.0395\n",
      "Epoch [  84/320], Step [ 104/104], Loss: 1.0704 Validation Loss: 1.0667\n",
      "Epoch [  85/320], Step [ 104/104], Loss: 0.7750 Validation Loss: 1.0232\n",
      "Epoch [  86/320], Step [ 104/104], Loss: 1.1477 Validation Loss: 0.9956\n",
      "Epoch [  87/320], Step [ 104/104], Loss: 1.1750 Validation Loss: 1.0147\n",
      "Epoch [  88/320], Step [ 104/104], Loss: 0.8623 Validation Loss: 1.0707\n",
      "Epoch [  89/320], Step [ 104/104], Loss: 1.4624 Validation Loss: 1.0735\n",
      "Epoch [  90/320], Step [ 104/104], Loss: 0.9095 Validation Loss: 1.0236\n",
      "Epoch [  91/320], Step [ 104/104], Loss: 0.9980 Validation Loss: 0.9877\n",
      "Epoch [  92/320], Step [ 104/104], Loss: 1.0034 Validation Loss: 1.0018\n",
      "Epoch [  93/320], Step [ 104/104], Loss: 1.1190 Validation Loss: 0.9785\n",
      "Epoch [  94/320], Step [ 104/104], Loss: 0.9221 Validation Loss: 1.0025\n",
      "Epoch [  95/320], Step [ 104/104], Loss: 0.8301 Validation Loss: 0.9727\n",
      "Epoch [  96/320], Step [ 104/104], Loss: 0.6774 Validation Loss: 1.0077\n",
      "Epoch [  97/320], Step [ 104/104], Loss: 1.3248 Validation Loss: 1.0693\n",
      "Epoch [  98/320], Step [ 104/104], Loss: 0.9647 Validation Loss: 0.9608\n",
      "Epoch [  99/320], Step [ 104/104], Loss: 0.8254 Validation Loss: 0.9453\n",
      "Epoch [ 100/320], Step [ 104/104], Loss: 1.1648 Validation Loss: 1.0043\n",
      "Epoch [ 101/320], Step [ 104/104], Loss: 0.9184 Validation Loss: 0.9648\n",
      "Epoch [ 102/320], Step [ 104/104], Loss: 0.7892 Validation Loss: 0.9675\n",
      "Epoch [ 103/320], Step [ 104/104], Loss: 0.9502 Validation Loss: 0.9583\n",
      "Epoch [ 104/320], Step [ 104/104], Loss: 1.3325 Validation Loss: 0.9765\n",
      "Epoch [ 105/320], Step [ 104/104], Loss: 1.2837 Validation Loss: 0.9646\n",
      "Epoch [ 106/320], Step [ 104/104], Loss: 1.0005 Validation Loss: 0.9708\n",
      "Epoch [ 107/320], Step [ 104/104], Loss: 1.2098 Validation Loss: 0.9689\n",
      "Epoch [ 108/320], Step [ 104/104], Loss: 0.9949 Validation Loss: 0.9788\n",
      "Epoch [ 109/320], Step [ 104/104], Loss: 1.2203 Validation Loss: 0.9534\n",
      "Epoch [ 110/320], Step [ 104/104], Loss: 0.7996 Validation Loss: 0.9451\n",
      "Epoch [ 111/320], Step [ 104/104], Loss: 1.2293 Validation Loss: 0.9366\n",
      "Epoch [ 112/320], Step [ 104/104], Loss: 0.8408 Validation Loss: 1.0269\n",
      "Epoch [ 113/320], Step [ 104/104], Loss: 1.0373 Validation Loss: 1.0083\n",
      "Epoch [ 114/320], Step [ 104/104], Loss: 1.0243 Validation Loss: 0.9295\n",
      "Epoch [ 115/320], Step [ 104/104], Loss: 0.7443 Validation Loss: 0.9249\n",
      "Epoch [ 116/320], Step [ 104/104], Loss: 0.9353 Validation Loss: 0.9057\n",
      "Epoch [ 117/320], Step [ 104/104], Loss: 0.9678 Validation Loss: 0.8959\n",
      "Epoch [ 118/320], Step [ 104/104], Loss: 0.6200 Validation Loss: 0.9137\n",
      "Epoch [ 119/320], Step [ 104/104], Loss: 1.0241 Validation Loss: 0.8796\n",
      "Epoch [ 120/320], Step [ 104/104], Loss: 0.9400 Validation Loss: 0.8878\n",
      "Epoch [ 121/320], Step [ 104/104], Loss: 1.0164 Validation Loss: 0.9384\n",
      "Epoch [ 122/320], Step [ 104/104], Loss: 0.8423 Validation Loss: 0.9142\n",
      "Epoch [ 123/320], Step [ 104/104], Loss: 0.9857 Validation Loss: 0.8845\n",
      "Epoch [ 124/320], Step [ 104/104], Loss: 0.6511 Validation Loss: 0.9016\n",
      "Epoch [ 125/320], Step [ 104/104], Loss: 0.9043 Validation Loss: 1.0221\n",
      "Epoch [ 126/320], Step [ 104/104], Loss: 1.0130 Validation Loss: 0.9652\n",
      "Epoch [ 127/320], Step [ 104/104], Loss: 0.9372 Validation Loss: 0.9031\n",
      "Epoch [ 128/320], Step [ 104/104], Loss: 1.0407 Validation Loss: 0.9122\n",
      "Epoch [ 129/320], Step [ 104/104], Loss: 1.2950 Validation Loss: 0.8995\n",
      "Epoch [ 130/320], Step [ 104/104], Loss: 0.9830 Validation Loss: 0.8943\n",
      "Epoch [ 131/320], Step [ 104/104], Loss: 0.7461 Validation Loss: 0.9383\n",
      "Epoch [ 132/320], Step [ 104/104], Loss: 1.0052 Validation Loss: 0.9345\n",
      "Epoch [ 133/320], Step [ 104/104], Loss: 0.6090 Validation Loss: 0.9472\n",
      "Epoch [ 134/320], Step [ 104/104], Loss: 0.6347 Validation Loss: 0.8634\n",
      "Epoch [ 135/320], Step [ 104/104], Loss: 1.0076 Validation Loss: 0.8798\n",
      "Epoch [ 136/320], Step [ 104/104], Loss: 0.7039 Validation Loss: 0.8735\n",
      "Epoch [ 137/320], Step [ 104/104], Loss: 1.0683 Validation Loss: 0.9413\n",
      "Epoch [ 138/320], Step [ 104/104], Loss: 0.8955 Validation Loss: 0.8402\n",
      "Epoch [ 139/320], Step [ 104/104], Loss: 0.9110 Validation Loss: 0.8855\n",
      "Epoch [ 140/320], Step [ 104/104], Loss: 0.8070 Validation Loss: 0.9064\n",
      "Epoch [ 141/320], Step [ 104/104], Loss: 0.8898 Validation Loss: 0.8946\n",
      "Epoch [ 142/320], Step [ 104/104], Loss: 0.8867 Validation Loss: 0.9224\n",
      "Epoch [ 143/320], Step [ 104/104], Loss: 1.1305 Validation Loss: 1.0443\n",
      "Epoch [ 144/320], Step [ 104/104], Loss: 1.0461 Validation Loss: 0.8936\n",
      "Epoch [ 145/320], Step [ 104/104], Loss: 0.9950 Validation Loss: 0.8647\n",
      "Epoch [ 146/320], Step [ 104/104], Loss: 0.8182 Validation Loss: 0.8704\n"
     ]
    }
   ],
   "source": [
    "# import lightning as L\n",
    "# trainer = L.Trainer(\n",
    "#     max_epochs=10,\n",
    "#     accelerator='cpu',\n",
    "#     log_every_n_steps=1        \n",
    "# )\n",
    "# trainer.fit(model=model, train_dataloaders=train_dl)\n",
    "\n",
    "optimizer, lr_scheduler = model_p.configure_optimizers(step_size=step_size, gamma=gamma, learning_rate=learning_rate)\n",
    "optimizer, lr_scheduler = optimizer[0], lr_scheduler[0]\n",
    "\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "best_val_loss = 500\n",
    "n_total_steps = len(train_dl)\n",
    "for epoch in range(num_epoch):\n",
    "    # Treinamento\n",
    "    model_p.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        loss = model_p.training_step(batch)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % math.floor(n_total_steps/printQtd) == 0:\n",
    "            print (f'Epoch [{epoch+1:4d}/{num_epoch}], Step [{i+1:4d}/{n_total_steps}], Loss: {loss.item():.4f}', end= \"\" if n_total_steps/printQtd+i >= n_total_steps else \"\\n\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Validação\n",
    "    model_p.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            val_loss += model_p.validation_step(batch)\n",
    "    \n",
    "    val_loss /= len(test_dl)\n",
    "    print(f' Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    train_errors.append(train_loss/len(train_dl))\n",
    "    validation_errors.append(val_loss.item())\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model_p.state_dict()  # Salva os parâmetros do modelo\n",
    "\n",
    "model_p.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accTotal = 0\n",
    "predicted_values = []\n",
    "real_values = []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(num_classes)]\n",
    "    n_class_samples = [0 for i in range(num_classes)]\n",
    "    n_each_class_samples = [0 for i in range(num_classes)]\n",
    "\n",
    "    for data, labels in test_dl:\n",
    "        outputs = model_p(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for pred, real in zip (predicted, labels):\n",
    "            predicted_values.append(pred.item())\n",
    "            real_values.append(real.item())\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            label = labels[i]\n",
    "            pred  = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            n_each_class_samples[pred] += 1\n",
    "\n",
    "    accTotal = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {accTotal} %')\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {test_ds.getLabel(i)} ({n_class_correct[i]}/{n_class_samples[i]} | {n_each_class_samples[i]}): {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salva resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p.save_backbone(accuracy=accTotal, batch_size=batch_size, num_epoch=num_epoch)\n",
    "\n",
    "pred_reports = pd.DataFrame({\n",
    "    Sets.REAL.value: real_values,\n",
    "    Sets.PREDICTION.value : predicted_values\n",
    "})\n",
    "pred_reports.to_csv(f\"{path_reports}/predictions_{ModelTypes.PRETEXT.value}.dat\", sep=\" \", index=False)\n",
    "\n",
    "train_reports = pd.DataFrame({\n",
    "    Sets.TRAIN.value : train_errors,\n",
    "    Sets.VALIDATION.value : validation_errors\n",
    "})\n",
    "train_reports.to_csv(f\"{path_reports}/errors_{ModelTypes.PRETEXT.value}.dat\", sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treina Downstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperparâmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_modules.har import HarDataModule as HarDataModuleDownstram\n",
    "from utils.enums import Datas, Sets, ModelTypes\n",
    "from models.cnn1d import CNN1d\n",
    "\n",
    "printQtd = 1\n",
    "isFreezing = False\n",
    "batch_size = 10\n",
    "num_epoch = 500\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optim config\n",
    "learning_rate_bb = 0.02\n",
    "step_size_bb = 120\n",
    "gamma_bb = 0.5\n",
    "\n",
    "learning_rate_ds = 0.01\n",
    "step_size_ds = 100\n",
    "gamma_ds = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrega Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = HarDataModuleDownstram(batch_size=batch_size)\n",
    "train_dl, train_ds = data_module.get_dataloader(set=Sets.TRAIN.value, shuffle=True)\n",
    "test_dl, test_ds   = data_module.get_dataloader(set=Sets.TEST.value, shuffle=True)\n",
    "num_classes = len(train_ds.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1d(\n",
    "    data_label=main_data.value, \n",
    "    num_classes=num_classes, \n",
    "    require_grad= not isFreezing, \n",
    "    type=ModelTypes.DOWNSTREAM.value\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightning as L\n",
    "# trainer = L.Trainer(\n",
    "#     max_epochs=10,\n",
    "#     accelerator='cpu',\n",
    "#     log_every_n_steps=1        \n",
    "# )\n",
    "# trainer.fit(model=model, train_dataloaders=train_dl)\n",
    "\n",
    "optimizer_backbone, lr_scheduler_backbone = model.configure_backbone_optimizers(step_size=step_size_bb, gamma=gamma_bb, learning_rate=learning_rate_bb)\n",
    "optimizer_downstream, lr_scheduler_downstream = model.configure_head_optimizers(step_size=step_size_ds, gamma=gamma_ds, learning_rate=learning_rate_ds)\n",
    "\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "best_val_loss = 500\n",
    "n_total_steps = len(train_dl)\n",
    "for epoch in range(num_epoch):\n",
    "    # Treinamento\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        loss = model.training_step(batch)\n",
    "        train_loss += loss.item()\n",
    "        optimizer_backbone.zero_grad()\n",
    "        optimizer_downstream.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_backbone.step()\n",
    "        optimizer_downstream.step()\n",
    "        \n",
    "        if (i+1) % math.floor(n_total_steps/printQtd) == 0:\n",
    "            print (f'Epoch [{epoch+1:4d}/{num_epoch}], Step [{i+1:4d}/{n_total_steps}], Loss: {loss.item():.4f}', end= \"\" if n_total_steps/printQtd+i >= n_total_steps else \"\\n\")\n",
    "\n",
    "    lr_scheduler_backbone.step()\n",
    "    lr_scheduler_downstream.step()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            val_loss += model.validation_step(batch)\n",
    "    \n",
    "    val_loss /= len(test_dl)\n",
    "    print(f' Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    train_errors.append(train_loss/len(train_dl))\n",
    "    validation_errors.append(val_loss.item())\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict()  # Salva os parâmetros do modelo\n",
    "\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accTotal = 0\n",
    "predicted_values = []\n",
    "real_values = []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(num_classes)]\n",
    "    n_class_samples = [0 for i in range(num_classes)]\n",
    "    n_each_class_samples = [0 for i in range(num_classes)]\n",
    "\n",
    "    for data, labels in test_dl:\n",
    "        outputs = model(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for pred, real in zip (predicted, labels):\n",
    "            predicted_values.append(pred.item())\n",
    "            real_values.append(real.item())\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            label = labels[i]\n",
    "            pred  = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            n_each_class_samples[pred] += 1\n",
    "\n",
    "    accTotal = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {accTotal} %')\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {test_ds.getLabel(i)} ({n_class_correct[i]}/{n_class_samples[i]} | {n_each_class_samples[i]}): {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salva Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_full_model(accuracy=accTotal, batch_size=batch_size, num_epoch=num_epoch)\n",
    "\n",
    "pred_reports = pd.DataFrame({\n",
    "    Sets.REAL.value: real_values,\n",
    "    Sets.PREDICTION.value : predicted_values\n",
    "})\n",
    "pred_reports.to_csv(f\"{path_reports}/predictions_{ModelTypes.DOWNSTREAM.value}.dat\", sep=\" \", index=False)\n",
    "\n",
    "train_reports = pd.DataFrame({\n",
    "    Sets.TRAIN.value : train_errors,\n",
    "    Sets.VALIDATION.value : validation_errors\n",
    "})\n",
    "train_reports.to_csv(f\"{path_reports}/errors_{ModelTypes.DOWNSTREAM.value}.dat\", sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realiza treinamento com percentuais da base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperparâmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_modules.har import HarDataModule as HarDataModuleDownstram\n",
    "from utils.enums import Datas, Sets, ModelTypes\n",
    "from models.cnn1d import CNN1d\n",
    "\n",
    "printQtd = 1\n",
    "isFreezing = False\n",
    "batch_size = 1\n",
    "num_epoch = 1000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Optim config\n",
    "learning_rate = 0.02\n",
    "step_size = 120\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realiza Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = [1, 20, 40, 60, 80, 100]\n",
    "\n",
    "for percent in percents:\n",
    "\n",
    "    # Carrega a base de dados\n",
    "    data_module = HarDataModuleDownstram(batch_size=64)\n",
    "    train_dl, train_ds = data_module.get_dataloader(set=Sets.TRAIN.value, shuffle=True, percent=percent/100)\n",
    "    test_dl, test_ds   = data_module.get_dataloader(set=Sets.TEST.value, shuffle=True)\n",
    "    num_classes = len(test_ds.labels)\n",
    "\n",
    "    # Define os modelos\n",
    "    model1 = CNN1d(\n",
    "        data_label=main_data.value, \n",
    "        num_classes=num_classes, \n",
    "        require_grad= not isFreezing, \n",
    "        type=ModelTypes.DOWNSTREAM.value\n",
    "    )\n",
    "    model2 = CNN1d(\n",
    "        data_label=main_data.value, \n",
    "        num_classes=num_classes, \n",
    "        require_grad=True, \n",
    "        type=ModelTypes.PRETEXT.value\n",
    "    )\n",
    "\n",
    "    # Define os otimizadores\n",
    "    optimizer1, lr_scheduler1 = model1.configure_optimizers(step_size=step_size, gamma=gamma, learning_rate=learning_rate)\n",
    "    optimizer2, lr_scheduler2 = model2.configure_optimizers(step_size=step_size, gamma=gamma, learning_rate=learning_rate)\n",
    "\n",
    "    optimizer1, lr_scheduler1 = optimizer1[0], lr_scheduler1[0]\n",
    "    optimizer2, lr_scheduler2 = optimizer2[0], lr_scheduler2[0]\n",
    "\n",
    "    # Realiza o treinamento\n",
    "    best_val_loss1 = 500\n",
    "    best_val_loss2 = 500\n",
    "    n_total_steps = len(train_dl)\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        # Treinamento \n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        for i, batch in enumerate(train_dl):\n",
    "            if len(batch) <= 0:\n",
    "                break\n",
    "            loss1 = model1.training_step(batch)\n",
    "            loss2 = model2.training_step(batch)\n",
    "\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "            loss1.backward()\n",
    "            loss2.backward()\n",
    "\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "\n",
    "        lr_scheduler1.step()\n",
    "        lr_scheduler2.step()\n",
    "\n",
    "        # Validação\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "\n",
    "        val_loss1 = 0\n",
    "        val_loss2 = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dl:\n",
    "                val_loss1 += model1.validation_step(batch)\n",
    "                val_loss2 += model2.validation_step(batch)\n",
    "        \n",
    "        val_loss1 /= len(test_dl)\n",
    "        val_loss2 /= len(test_dl)\n",
    "\n",
    "        if val_loss1 < best_val_loss1:\n",
    "            best_val_loss1 = val_loss1\n",
    "            best_model1 = model1.state_dict() \n",
    "\n",
    "        if val_loss2 < best_val_loss2:\n",
    "            best_val_loss2 = val_loss2\n",
    "            best_model2 = model2.state_dict()  \n",
    "\n",
    "    model1.load_state_dict(best_model1)\n",
    "    model2.load_state_dict(best_model2)\n",
    "\n",
    "    predicted_values1 = []\n",
    "    predicted_values2 = []\n",
    "\n",
    "    real_values1 = []\n",
    "    real_values2 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dl:\n",
    "            outputs1 = model1(data)\n",
    "            _, predicted1 = torch.max(outputs1, 1)\n",
    "\n",
    "            for pred, real in zip (predicted1, labels):\n",
    "                predicted_values1.append(pred.item())\n",
    "                real_values1.append(real.item())\n",
    "\n",
    "            outputs2 = model2(data)\n",
    "            _, predicted2 = torch.max(outputs2, 1)\n",
    "\n",
    "            for pred, real in zip (predicted2, labels):\n",
    "                predicted_values2.append(pred.item())\n",
    "                real_values2.append(real.item())\n",
    "\n",
    "    pred_reports1 = pd.DataFrame({\n",
    "        Sets.REAL.value: real_values1,\n",
    "        Sets.PREDICTION.value : predicted_values1\n",
    "    })\n",
    "    pred_reports2 = pd.DataFrame({\n",
    "        Sets.REAL.value: real_values2,\n",
    "        Sets.PREDICTION.value : predicted_values2\n",
    "    })\n",
    "\n",
    "    pred_reports1.to_csv(f\"{path_reports}/predictions_{percent}_m1.dat\", sep=\" \", index=False)\n",
    "    pred_reports2.to_csv(f\"{path_reports}/predictions_{percent}_m2.dat\", sep=\" \", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
